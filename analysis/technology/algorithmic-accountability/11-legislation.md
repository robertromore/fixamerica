# Algorithmic Accountability: Legislation and Legal Framework

## Overview

Comprehensive algorithmic accountability legislation must establish transparency requirements, mandate impact assessments, apply anti-discrimination standards, provide redress mechanisms, and ensure meaningful enforcement. This document provides draft legislation for key reforms.

## Federal Legislation

### Algorithmic Accountability Act

**Purpose**: Establish comprehensive requirements for automated decision systems affecting Americans.

**Draft Text**:

```text
SECTION 1. SHORT TITLE.

This Act may be cited as the "Algorithmic Accountability Act of 2026".

SEC. 2. FINDINGS.

Congress finds that—
    (1) automated decision systems are increasingly used to make
    consequential decisions about Americans;
    (2) these systems can perpetuate and amplify discrimination;
    (3) many systems operate without transparency or accountability;
    (4) existing legal frameworks are insufficient; and
    (5) comprehensive legislation is necessary.

SEC. 3. DEFINITIONS.

(a) AUTOMATED DECISION SYSTEM.—The term "automated decision system"
means any computational process that uses machine learning, statistical
modeling, data analytics, or artificial intelligence techniques to
make or materially assist in making decisions, predictions, or
classifications affecting individuals.

(b) COVERED ENTITY.—The term "covered entity" means any person that—
    (1) develops, deploys, or uses an automated decision system; and
    (2) meets the size thresholds established by the Commission.

(c) HIGH-RISK AUTOMATED DECISION SYSTEM.—The term "high-risk
automated decision system" means an automated decision system that—
    (1) makes or materially assists in making consequential decisions
    affecting individuals in the areas of—
        (A) employment, including hiring, compensation, and termination;
        (B) credit, insurance, and financial services;
        (C) housing;
        (D) healthcare;
        (E) education;
        (F) criminal justice;
        (G) government benefits and services; or
        (H) other areas designated by the Commission; and
    (2) may pose a significant risk of harm to individuals based on—
        (A) the nature of the decision;
        (B) the number of individuals affected; or
        (C) the vulnerability of affected populations.

(d) CONSEQUENTIAL DECISION.—The term "consequential decision" means
a decision that has a material legal or similarly significant effect
on an individual's access to, terms of, or eligibility for—
    (1) employment;
    (2) credit or insurance;
    (3) housing;
    (4) healthcare;
    (5) education;
    (6) government benefits; or
    (7) similar goods, services, or opportunities.

SEC. 4. IMPACT ASSESSMENTS.

(a) REQUIREMENT.—Before deploying a high-risk automated decision
system, a covered entity shall conduct an algorithmic impact
assessment.

(b) CONTENTS.—An impact assessment shall include—
    (1) a description of the system and its purpose;
    (2) an analysis of potential harms, including—
        (A) discriminatory impacts on protected classes;
        (B) errors and their distribution across populations;
        (C) privacy impacts; and
        (D) other harms;
    (3) an evaluation of less harmful alternatives;
    (4) testing results for bias and accuracy;
    (5) a mitigation plan for identified risks; and
    (6) an ongoing monitoring plan.

(c) UPDATES.—Impact assessments shall be updated—
    (1) at least annually;
    (2) when material changes are made; or
    (3) when significant issues are identified.

(d) SUBMISSION.—Impact assessments for high-risk systems shall be
submitted to the Commission.

SEC. 5. TRANSPARENCY REQUIREMENTS.

(a) NOTICE.—A covered entity that uses an automated decision system
to make a consequential decision shall provide clear and conspicuous
notice that such a system is being used.

(b) DISCLOSURE.—When a consequential decision is made with material
assistance from an automated decision system, the covered entity shall
disclose to the affected individual—
    (1) that an automated decision system was used;
    (2) the role of the system in the decision;
    (3) the principal factors that led to the decision; and
    (4) information about how to contest the decision.

(c) PUBLIC DISCLOSURE.—Covered entities shall publicly disclose—
    (1) a list of high-risk automated decision systems in use;
    (2) summary information about system purposes and impacts; and
    (3) aggregate demographic data on system outcomes.

SEC. 6. ANTI-DISCRIMINATION REQUIREMENTS.

(a) PROHIBITION.—It shall be unlawful for a covered entity to use
an automated decision system in a manner that—
    (1) discriminates against any individual on the basis of race,
    color, religion, national origin, sex (including sexual
    orientation and gender identity), age, disability, or genetic
    information; or
    (2) has a disparate impact on any such protected class, unless—
        (A) the practice is job-related and consistent with business
        necessity (for employment);
        (B) the practice serves a legitimate, non-discriminatory
        purpose that cannot be achieved through less discriminatory
        means (for other contexts).

(b) TESTING.—Covered entities shall regularly test automated decision
systems for disparate impact.

(c) MITIGATION.—When disparate impact is identified, covered entities
shall take reasonable steps to mitigate the impact.

SEC. 7. REDRESS.

(a) RIGHT TO EXPLANATION.—An individual subject to a consequential
decision made with an automated decision system has the right to—
    (1) meaningful information about the logic involved; and
    (2) the principal factors that led to the decision.

(b) RIGHT TO CONTEST.—An individual has the right to contest any
consequential decision and receive—
    (1) reconsideration by a human decision-maker;
    (2) the opportunity to present additional information; and
    (3) a written explanation of the reconsideration outcome.

(c) RIGHT TO CORRECTION.—An individual has the right to correction
of inaccurate data used in automated decision systems.

SEC. 8. ENFORCEMENT.

(a) FTC ENFORCEMENT.—The Federal Trade Commission shall enforce
this Act under Section 5 of the FTC Act.

(b) RULEMAKING.—The Commission shall issue regulations implementing
this Act within 2 years of enactment.

(c) CIVIL PENALTIES.—
    (1) Violations of this Act are subject to civil penalties of—
        (A) up to $50,000 per violation for impact assessment failures;
        (B) up to $10,000 per violation for transparency failures; and
        (C) up to $100,000 per violation for discrimination violations.
    (2) Knowing violations are subject to treble damages.

(d) PRIVATE RIGHT OF ACTION.—
    (1) Any individual harmed by a violation of this Act may bring
    a civil action for—
        (A) actual damages or $1,000 per violation, whichever is
        greater;
        (B) injunctive relief;
        (C) punitive damages for knowing violations; and
        (D) reasonable attorney's fees.
    (2) Class actions are permitted.

(e) STATE ENFORCEMENT.—State attorneys general may bring actions
to enforce this Act.

(f) RELATIONSHIP TO OTHER LAWS.—This Act supplements and does not
preempt other federal or state laws providing greater protection.

SEC. 9. GOVERNMENT USE.

(a) REQUIREMENTS.—Federal agencies using automated decision systems
shall—
    (1) conduct impact assessments;
    (2) provide public notice;
    (3) ensure due process; and
    (4) conduct regular audits.

(b) PROCUREMENT.—Federal procurement of automated decision systems
shall require vendor compliance with this Act.

(c) HIGH-STAKES DECISIONS.—Automated decision systems shall not be
the sole basis for decisions that significantly affect individual
rights or access to government benefits.

SEC. 10. EFFECTIVE DATE.

This Act takes effect 2 years after enactment, with impact assessment
requirements effective 1 year after enactment.
```

### Civil Rights Amendments for Algorithms

**Purpose**: Clarify application of civil rights laws to algorithmic discrimination.

**Draft Text**:

```text
SECTION 1. SHORT TITLE.

This Act may be cited as the "Civil Rights Amendments for the
Algorithmic Age Act of 2025".

SEC. 2. TITLE VII AMENDMENTS.

Section 703 of the Civil Rights Act of 1964 (42 U.S.C. § 2000e-2)
is amended by adding at the end the following:

"(o) AUTOMATED DECISION SYSTEMS.—
    "(1) It shall be an unlawful employment practice for an employer
    to use an automated decision system in a manner that discriminates
    against any individual because of such individual's race, color,
    religion, sex, or national origin, or that has a disparate impact
    on the basis of such characteristics.

    "(2) An employer that uses an automated decision system for
    employment decisions shall—
        "(A) evaluate the system for disparate impact before use
        and periodically thereafter;
        "(B) take reasonable steps to mitigate identified disparities;
        "(C) maintain records of such evaluations; and
        "(D) make information available to the Commission upon request.

    "(3) A covered entity may be held liable for discrimination
    resulting from an automated decision system whether or not the
    system was developed by a third party."

SEC. 3. FAIR HOUSING ACT AMENDMENTS.

Section 804 of the Fair Housing Act (42 U.S.C. § 3604) is amended
by adding at the end the following:

"(g) AUTOMATED DECISION SYSTEMS.—It shall be unlawful to use an
automated decision system in connection with the sale, rental, or
financing of housing in a manner that discriminates or has a
disparate impact on the basis of race, color, religion, sex,
familial status, national origin, or disability."

SEC. 4. EQUAL CREDIT OPPORTUNITY ACT AMENDMENTS.

Section 701 of the Equal Credit Opportunity Act (15 U.S.C. § 1691)
is amended by adding at the end the following:

"(g) AUTOMATED DECISION SYSTEMS.—
    "(1) A creditor shall not use an automated decision system in
    a manner that discriminates or has a disparate impact on the
    basis of race, color, religion, national origin, sex, marital
    status, age, or receipt of public assistance.

    "(2) When an adverse action is taken with material assistance
    from an automated decision system, the statement of reasons
    shall include the principal factors in the system that
    contributed to the action."
```

## State Model Legislation

### Model State Algorithmic Accountability Act

```text
SECTION 1. SHORT TITLE.

This Act may be cited as the "[State] Algorithmic Accountability Act".

SECTION 2. DEFINITIONS.

(a) "Automated decision system" means any computational process that
uses machine learning, artificial intelligence, or data analytics to
make or materially assist in making decisions affecting individuals.

(b) "Consequential decision" means a decision that has a material
effect on an individual's access to or terms of—
    (1) employment;
    (2) credit, insurance, or financial services;
    (3) housing;
    (4) healthcare;
    (5) education; or
    (6) government benefits.

(c) "Deployer" means any person that uses an automated decision
system to make or assist in making consequential decisions.

SECTION 3. IMPACT ASSESSMENTS.

(a) A deployer of an automated decision system for consequential
decisions shall conduct an impact assessment that includes—
    (1) system purpose and description;
    (2) evaluation for discriminatory effects;
    (3) accuracy and error analysis;
    (4) risk mitigation plan.

(b) Assessments shall be updated annually or when material changes
are made.

SECTION 4. TRANSPARENCY.

(a) A deployer shall provide notice when an automated decision
system is used for consequential decisions.

(b) Upon request, a deployer shall provide an explanation of the
principal factors in a decision.

SECTION 5. ANTI-DISCRIMINATION.

(a) A deployer shall not use an automated decision system in a
manner that discriminates on the basis of protected class.

(b) Disparate impact constitutes discrimination unless the practice
is necessary and cannot be achieved through less discriminatory means.

SECTION 6. REDRESS.

(a) Individuals have the right to contest decisions made with
automated decision systems.

(b) A human decision-maker shall review contested decisions.

SECTION 7. ENFORCEMENT.

(a) The [Attorney General] may bring actions for violations.

(b) Civil penalties of up to $[X] per violation apply.

(c) Individuals may bring private actions for actual damages or
$[X] per violation, plus attorney's fees.

SECTION 8. EFFECTIVE DATE.

This Act takes effect [date].
```

### Model State AI Hiring Transparency Act

```text
SECTION 1. SHORT TITLE.

This Act may be cited as the "[State] AI Hiring Transparency Act".

SECTION 2. DEFINITIONS.

(a) "Automated employment decision tool" means any computational
process that substantially assists or replaces discretionary
decision-making in employment decisions.

SECTION 3. REQUIREMENTS.

(a) Before using an automated employment decision tool, an employer
shall—
    (1) conduct a bias audit within the past year;
    (2) make a summary of the audit publicly available; and
    (3) provide notice to candidates of the tool's use.

(b) The bias audit shall—
    (1) be conducted by an independent auditor;
    (2) test for disparate impact by protected class; and
    (3) include adverse impact ratios.

SECTION 4. NOTICE.

(a) At least 10 days before use, an employer shall provide notice—
    (1) that an automated tool will be used;
    (2) of the job qualifications assessed; and
    (3) of the data to be collected.

SECTION 5. ENFORCEMENT.

(a) Civil penalties of $[X] for first violation and $[X] for each
subsequent violation.

(b) Each day of continued use in violation constitutes a separate
violation.

SECTION 6. EFFECTIVE DATE.

This Act takes effect [date].
```

## Constitutional Considerations

### Due Process

| Issue | Analysis |
|-------|----------|
| **Notice** | Transparency requirements satisfy |
| **Hearing** | Redress mechanisms provide opportunity |
| **Fairness** | Accuracy requirements support |
| **Government use** | Additional constitutional constraints |

### Equal Protection

| Issue | Analysis |
|-------|----------|
| **Disparate treatment** | Prohibited clearly |
| **Disparate impact** | Standard established |
| **Intent** | Not required for statutory claims |
| **Strict scrutiny** | For suspect classifications |

### First Amendment

| Issue | Analysis |
|-------|----------|
| **Compelled speech** | Limited, commercial context |
| **Commercial speech** | Lower scrutiny |
| **Expressive conduct** | Algorithms not expressive |

### Fourth Amendment

| Issue | Analysis |
|-------|----------|
| **Government use** | Privacy constraints |
| **Data collection** | Reasonable limits permissible |

## Loopholes, Shortcomings, and Rectification

### Potential Loopholes

| Loophole | Description | Severity |
|----------|-------------|----------|
| **Threshold avoidance** | Structure to avoid coverage | Medium |
| **Human-in-the-loop gaming** | Nominal human involvement | High |
| **Third-party shifting** | Blame vendors | High |
| **Trade secret claims** | Block transparency | Medium |
| **Consent extraction** | Waive protections | Medium |

### Shortcomings

| Issue | Impact | Root Cause |
|-------|--------|------------|
| **Enforcement resources** | Limited capacity | Budget |
| **Technical complexity** | Verification hard | Expertise |
| **Evolving technology** | Law lags | Speed |
| **Fairness definition** | Multiple metrics | Technical |
| **Standing issues** | Harm proof difficult | Legal |

### Rectification Procedures

1. **Functional coverage tests** based on actual impact
2. **Meaningful human review** standards
3. **Chain of liability** through supply chain
4. **Qualified transparency** balancing trade secrets
5. **Anti-waiver provisions** for core rights
6. **Adequate enforcement funding** tied to industry size
7. **Technical advisory capacity** for regulators
8. **Regular updates** to address new technology
9. **Multiple fairness metrics** required
10. **Presumed harm** for certain violations

## References

### Statutory References

- Civil Rights Act of 1964, 42 U.S.C. § 2000e
- Fair Housing Act, 42 U.S.C. § 3601
- Equal Credit Opportunity Act, 15 U.S.C. § 1691
- Fair Credit Reporting Act, 15 U.S.C. § 1681
- Americans with Disabilities Act, 42 U.S.C. § 12101
- NYC Local Law 144 (2021)
- Illinois AI Video Interview Act (2019)

### Key Cases

- *Griggs v. Duke Power Co.*, 401 U.S. 424 (1971)
- *Texas Dept. of Housing v. Inclusive Communities*, 576 U.S. 519 (2015)
- *Loomis v. Wisconsin*, 881 N.W.2d 749 (Wis. 2016)
- *TransUnion v. Ramirez*, 141 S. Ct. 2190 (2021)

### Policy Documents

- Blueprint for an AI Bill of Rights, OSTP (2022)
- NIST AI Risk Management Framework
- EEOC AI guidance documents
- EU AI Act

## Related Topics

- [Artificial Intelligence: Legislation](../artificial-intelligence/11-legislation.md)
- [Data Privacy: Legislation](../data-privacy/11-legislation.md)
- [Civil Rights Enforcement: Legislation](../../justice/civil-rights-enforcement/11-legislation.md)

---

## Document Navigation

- Previous: [Actions](10-actions.md)
- Up: [Overview](01-overview.md)
