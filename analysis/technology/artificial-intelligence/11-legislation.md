# Artificial Intelligence: Legislation and Legal Framework

## Overview

AI governance requires a multi-layered legal framework addressing safety, discrimination, transparency, and accountability. This document provides draft legislation for comprehensive federal AI governance, with supporting state model legislation and regulatory frameworks.

## Federal Legislation

### Comprehensive AI Governance Act

**Purpose**: Establish a baseline federal framework for AI oversight, including a lead agency, risk classification, and enforceable requirements.

**Draft Text**:

```text
SECTION 1. SHORT TITLE.

This Act may be cited as the "Comprehensive AI Governance Act of 2025".

SEC. 2. FINDINGS.

Congress finds that—
    (1) artificial intelligence systems are increasingly used in
    consequential decisions affecting employment, credit, housing,
    healthcare, and criminal justice;
    (2) AI systems can perpetuate and amplify discrimination;
    (3) advanced AI systems pose potential safety and security risks;
    (4) the current regulatory framework is fragmented and inadequate;
    (5) public trust in AI requires transparency and accountability;
    (6) the United States must balance innovation with protection; and
    (7) federal action is necessary to establish consistent standards.

SEC. 3. DEFINITIONS.

(a) ARTIFICIAL INTELLIGENCE SYSTEM.—The term "artificial intelligence
system" means a machine-based system that—
    (1) is designed to operate with varying levels of autonomy;
    (2) may exhibit adaptiveness after deployment; and
    (3) for explicit or implicit objectives, infers from the input it
    receives how to generate outputs such as predictions, content,
    recommendations, or decisions that can influence physical or
    virtual environments.

(b) COVERED ENTITY.—The term "covered entity" means any person or
entity that develops, deploys, or operates an AI system, excluding—
    (1) federal, state, and local governments (covered separately);
    (2) natural persons using AI for personal, non-commercial purposes.

(c) DEPLOYER.—The term "deployer" means a covered entity that uses
an AI system to make or materially assist in making a consequential
decision.

(d) DEVELOPER.—The term "developer" means a covered entity that
designs, codes, or substantially modifies an AI system.

(e) HIGH-RISK AI SYSTEM.—The term "high-risk AI system" means an
AI system used in any of the following contexts:
    (1) Employment and worker management decisions;
    (2) Educational admission or opportunity decisions;
    (3) Credit, lending, and insurance determinations;
    (4) Housing eligibility and rental decisions;
    (5) Healthcare treatment recommendations or diagnostics;
    (6) Criminal justice, including pretrial risk assessment,
    sentencing, and parole decisions;
    (7) Public benefits eligibility determinations;
    (8) Critical infrastructure operation;
    (9) Biometric identification in public spaces; and
    (10) Other contexts designated by the Administrator.

(f) FRONTIER AI SYSTEM.—The term "frontier AI system" means an AI
system that—
    (1) was trained using a quantity of computing power greater than
    10^26 floating point operations; or
    (2) exhibits capabilities designated by the Administrator as
    posing potential catastrophic risks.

SEC. 4. ESTABLISHMENT OF AI ADMINISTRATION.

(a) ESTABLISHMENT.—There is established as an independent agency
of the United States the Artificial Intelligence Administration
(referred to as the "Administration").

(b) ADMINISTRATOR.—The Administration shall be headed by an
Administrator appointed by the President, by and with the advice
and consent of the Senate, for a term of 5 years.

(c) QUALIFICATIONS.—The Administrator shall have—
    (1) demonstrated expertise in artificial intelligence, technology
    policy, or a related field; and
    (2) experience in government, academia, or the private sector
    relevant to AI governance.

(d) FUNCTIONS.—The Administration shall—
    (1) issue regulations implementing this Act;
    (2) coordinate AI policy across federal agencies;
    (3) maintain a registry of high-risk AI systems;
    (4) conduct or commission AI safety research;
    (5) develop standards and guidance for AI systems;
    (6) enforce this Act and impose penalties for violations;
    (7) represent the United States in international AI governance;
    (8) report annually to Congress on AI developments and governance.

SEC. 5. HIGH-RISK AI REQUIREMENTS.

(a) IMPACT ASSESSMENT.—Before deploying a high-risk AI system, a
deployer shall—
    (1) conduct an impact assessment evaluating—
        (A) the purpose and intended use of the system;
        (B) the data used to train and operate the system;
        (C) potential discriminatory impacts;
        (D) privacy implications;
        (E) accuracy and reliability; and
        (F) safeguards and human oversight mechanisms;
    (2) submit the impact assessment to the Administration; and
    (3) make a summary of the assessment publicly available.

(b) ONGOING MONITORING.—A deployer of a high-risk AI system shall—
    (1) monitor the system for accuracy, bias, and other performance
    metrics;
    (2) maintain records of system outputs and decisions;
    (3) report significant incidents to the Administration; and
    (4) conduct annual reassessments.

(c) HUMAN OVERSIGHT.—High-risk AI systems shall be designed and
deployed to allow—
    (1) human review of consequential decisions;
    (2) the ability to override system outputs;
    (3) intervention to prevent or correct errors.

(d) TRANSPARENCY.—When a high-risk AI system makes or materially
assists in a decision affecting an individual, the deployer shall—
    (1) notify the individual that AI was used;
    (2) provide a meaningful explanation of the basis for the decision;
    (3) inform the individual of their right to contest the decision.

SEC. 6. FRONTIER AI REQUIREMENTS.

(a) PRE-DEPLOYMENT SAFETY TESTING.—Before deploying a frontier AI
system, a developer shall—
    (1) conduct safety evaluations for—
        (A) dangerous capabilities, including cyber, biological,
        chemical, nuclear, and radiological risks;
        (B) the system's ability to evade oversight;
        (C) potential for misuse by malicious actors;
        (D) risks of loss of control; and
    (2) submit a safety report to the Administration.

(b) REPORTING REQUIREMENTS.—A developer of a frontier AI system
shall report to the Administration—
    (1) the initiation of any training run meeting the frontier
    threshold, within 48 hours of commencement;
    (2) significant capability advances discovered during or after
    training;
    (3) any safety incidents or near-misses.

(c) SAFETY PLAN.—A developer of a frontier AI system shall maintain
and implement a safety plan that includes—
    (1) procedures for ongoing safety evaluation;
    (2) protocols for responding to safety incidents;
    (3) mechanisms for human oversight and intervention;
    (4) security measures to prevent unauthorized access.

(d) COMPLIANCE CERTIFICATION.—A developer shall certify annually
that the frontier AI system complies with safety requirements.

SEC. 7. PROHIBITED PRACTICES.

(a) PROHIBITIONS.—No covered entity shall deploy an AI system that—
    (1) is designed to manipulate human behavior in a manner that
    causes or is likely to cause significant harm;
    (2) exploits vulnerabilities of specific groups, including
    children, elderly persons, or persons with disabilities;
    (3) enables social scoring by public authorities;
    (4) engages in real-time remote biometric identification in
    publicly accessible spaces for law enforcement, except—
        (A) targeted searches for missing persons;
        (B) prevention of imminent threat to life; or
        (C) pursuant to judicial authorization;
    (5) infers sensitive characteristics, including race, religion,
    sexual orientation, or political affiliation, except where
    necessary for medical purposes or to detect discrimination.

SEC. 8. NON-DISCRIMINATION.

(a) PROHIBITION.—No covered entity shall deploy an AI system that
discriminates against any individual on the basis of race, color,
religion, sex (including pregnancy, sexual orientation, and gender
identity), national origin, age, disability, or genetic information.

(b) DISPARATE IMPACT.—An AI system shall be considered discriminatory
if its use results in a disparate impact on a protected class unless
the covered entity demonstrates that the system is necessary to
achieve a substantial, legitimate, and non-discriminatory interest
and no less discriminatory alternative exists.

(c) BIAS AUDITS.—Deployers of high-risk AI systems shall conduct
or commission annual bias audits by qualified independent auditors.

SEC. 9. DATA GOVERNANCE.

(a) DATA QUALITY.—Developers and deployers of high-risk AI systems
shall ensure that training and input data are—
    (1) relevant to the intended purpose;
    (2) sufficiently representative;
    (3) as free from errors and bias as reasonably achievable;
    (4) subject to appropriate governance measures.

(b) DATA DOCUMENTATION.—Developers shall document—
    (1) the sources of training data;
    (2) data collection and labeling methods;
    (3) known limitations and biases;
    (4) measures taken to address data quality issues.

SEC. 10. INDIVIDUAL RIGHTS.

(a) RIGHT TO KNOW.—Individuals have the right to know when an AI
system has been used in a decision that significantly affects them.

(b) RIGHT TO EXPLANATION.—Individuals have the right to receive a
meaningful explanation of the basis for an AI-assisted decision.

(c) RIGHT TO CONTEST.—Individuals have the right to contest an
AI-assisted decision and obtain human review.

(d) RIGHT TO NON-DISCRIMINATION.—Individuals have the right to be
free from unlawful discrimination by AI systems.

SEC. 11. ENFORCEMENT.

(a) ADMINISTRATIVE ENFORCEMENT.—The Administrator may—
    (1) investigate potential violations of this Act;
    (2) issue subpoenas for documents and testimony;
    (3) conduct audits of AI systems;
    (4) issue orders to cease violations;
    (5) impose civil penalties of up to $50,000 per violation per
    day, or 4 percent of annual global revenue, whichever is greater;
    (6) require corrective action.

(b) PRIVATE RIGHT OF ACTION.—Any individual whose rights under
this Act have been violated may bring a civil action for—
    (1) actual damages or $10,000 per violation, whichever is greater;
    (2) punitive damages for willful violations;
    (3) injunctive and declaratory relief; and
    (4) reasonable attorney's fees and costs.

(c) STATE ENFORCEMENT.—State attorneys general may bring actions
to enforce this Act on behalf of state residents.

(d) WHISTLEBLOWER PROTECTIONS.—No covered entity may retaliate
against any employee who reports a violation of this Act.

SEC. 12. STATE RELATIONSHIP.

(a) FLOOR, NOT CEILING.—This Act establishes minimum standards and
shall not preempt State laws that provide greater protections.

(b) STATE AUTHORITY.—States may enact and enforce laws governing
AI systems that exceed the requirements of this Act.

SEC. 13. RESEARCH AND DEVELOPMENT.

(a) AI SAFETY RESEARCH.—There is authorized to be appropriated
$500,000,000 annually for AI safety research.

(b) TESTING INFRASTRUCTURE.—The Administration shall establish
or contract for AI testing and evaluation infrastructure.

SEC. 14. INTERNATIONAL COORDINATION.

(a) The Administration shall coordinate with international partners
on AI governance, including through—
    (1) bilateral and multilateral agreements;
    (2) participation in international standards bodies;
    (3) mutual recognition of compliance frameworks.

SEC. 15. AUTHORIZATION OF APPROPRIATIONS.

There is authorized to be appropriated $300,000,000 annually for
the operation of the Administration.

SEC. 16. EFFECTIVE DATE.

(a) This Act shall take effect 180 days after enactment.

(b) Regulations implementing this Act shall be issued within 1 year.

(c) High-risk AI requirements shall apply 2 years after enactment.

(d) Frontier AI requirements shall apply 1 year after enactment.
```

### Frontier AI Safety Act

**Purpose**: Address specific risks from the most advanced AI systems.

**Draft Text**:

```text
SECTION 1. SHORT TITLE.

This Act may be cited as the "Frontier AI Safety Act of 2025".

SEC. 2. DEFINITIONS.

(a) FRONTIER AI MODEL.—A "frontier AI model" means a foundation model
trained using computational resources exceeding 10^26 integer or
floating-point operations, or that exhibits designated dangerous
capabilities.

(b) COVERED DEVELOPER.—Any entity that develops or substantially
modifies a frontier AI model.

SEC. 3. SAFETY REQUIREMENTS.

(a) BEFORE TRAINING.—A covered developer shall, before initiating
a frontier training run—
    (1) notify the Administration within 48 hours of commencement;
    (2) implement security measures to prevent model theft;
    (3) establish a safety and security protocol.

(b) BEFORE DEPLOYMENT.—A covered developer shall, before deploying
or enabling access to a frontier AI model—
    (1) conduct safety evaluations for dangerous capabilities;
    (2) implement safeguards proportionate to identified risks;
    (3) submit a safety report to the Administration;
    (4) certify that the model does not pose unreasonable risk.

(c) ONGOING REQUIREMENTS.—A covered developer shall—
    (1) maintain capability to shut down deployed models;
    (2) report safety incidents within 24 hours;
    (3) cooperate with government safety testing;
    (4) maintain records for 5 years.

SEC. 4. DANGEROUS CAPABILITIES.

(a) EVALUATION REQUIRED.—Frontier AI models shall be evaluated for—
    (1) ability to assist in creating weapons of mass destruction;
    (2) ability to conduct autonomous cyberattacks;
    (3) ability to evade human oversight or control;
    (4) ability to self-replicate or acquire resources autonomously;
    (5) other capabilities designated by the Administrator.

(b) RESPONSE.—If dangerous capabilities are identified, the covered
developer shall—
    (1) immediately notify the Administration;
    (2) implement containment measures;
    (3) not deploy until risks are adequately mitigated.

SEC. 5. ENFORCEMENT.

(a) PENALTIES.—Violations are subject to—
    (1) civil penalties up to $25,000,000 or 10 percent of annual
    revenue, whichever is greater;
    (2) criminal penalties for willful violations resulting in
    serious harm.

(b) INJUNCTIVE RELIEF.—The Administration may seek court orders to
prevent deployment of unsafe systems.

SEC. 6. WHISTLEBLOWER PROTECTIONS.

Strong protections for employees who report safety concerns.

SEC. 7. EFFECTIVE DATE.

This Act shall take effect 180 days after enactment.
```

## State Model Legislation

### Model State AI Accountability Act

```text
SECTION 1. SHORT TITLE.

This Act may be cited as the "[State] AI Accountability Act".

SECTION 2. DEFINITIONS.

(a) "Algorithmic discrimination" means the condition in which an
automated decision system contributes to unjustified differential
treatment or impacts based on a protected class.

(b) "Automated decision system" means a computational process that
uses machine learning or other techniques to make or substantially
assist in making consequential decisions.

(c) "Consequential decision" means a decision or judgment that has
a material legal or similarly significant effect on an individual's
access to or cost of employment, education, credit, housing,
healthcare, or public services.

(d) "Deployer" means a person doing business in this state that
deploys an automated decision system.

SECTION 3. IMPACT ASSESSMENT.

(a) Before deploying an automated decision system for consequential
decisions, a deployer shall—
    (1) conduct an impact assessment;
    (2) document the assessment; and
    (3) make the assessment available to the [Attorney General/
    designated agency] upon request.

(b) The impact assessment shall include—
    (1) a description of the system's purpose and intended use;
    (2) an analysis of potential algorithmic discrimination;
    (3) a description of training data and methodology;
    (4) an evaluation of the system's accuracy; and
    (5) a description of safeguards and human oversight.

SECTION 4. PROHIBITION ON ALGORITHMIC DISCRIMINATION.

(a) A deployer shall not use an automated decision system that
results in algorithmic discrimination.

(b) A deployer shall—
    (1) use reasonable care to avoid algorithmic discrimination;
    (2) conduct annual assessments for discrimination; and
    (3) take reasonable corrective action if discrimination is found.

SECTION 5. NOTICE AND TRANSPARENCY.

(a) A deployer shall provide notice to individuals when an automated
decision system is used in a consequential decision.

(b) Upon request, a deployer shall provide—
    (1) a description of the automated decision system;
    (2) the principal factors in the decision; and
    (3) information on how to contest the decision.

SECTION 6. ENFORCEMENT.

(a) The [Attorney General] may bring an action for—
    (1) injunctive relief;
    (2) civil penalties of $[X] per violation; and
    (3) costs and reasonable attorney's fees.

(b) A private right of action is established for individuals harmed
by violations, with remedies including—
    (1) actual damages or $[X] per violation, whichever is greater;
    (2) injunctive relief; and
    (3) reasonable attorney's fees.

SECTION 7. RELATIONSHIP TO FEDERAL LAW.

This Act shall not be construed to limit any rights or remedies
under federal law.

SECTION 8. EFFECTIVE DATE.

This Act shall take effect [date].
```

## Constitutional Considerations

### First Amendment

| Issue | Analysis |
|-------|----------|
| **AI as speech** | AI outputs may receive some protection |
| **Commercial speech** | Lower scrutiny for commercial AI |
| **Compelled disclosure** | Generally permissible for commercial |
| **Content restrictions** | Higher scrutiny; target conduct, not content |

### Due Process

| Issue | Analysis |
|-------|----------|
| **Procedural due process** | AI decisions affecting liberty/property require process |
| **Explanation requirements** | Consistent with due process |
| **Right to contest** | Supports due process values |
| **Algorithmic transparency** | May be required for meaningful review |

### Commerce Clause

| Issue | Analysis |
|-------|----------|
| **Interstate commerce** | Clear federal authority |
| **State regulation** | Cannot unduly burden interstate commerce |
| **Preemption** | Federal floor allows state action above |

## Loopholes, Shortcomings, and Rectification

### Potential Loopholes

| Loophole | Description | Severity |
|----------|-------------|----------|
| **Definition gaming** | Structure to avoid "AI system" definition | High |
| **Threshold manipulation** | Stay below frontier compute threshold | Medium |
| **Offshore development** | Develop outside US to avoid rules | High |
| **Subsidiary structures** | Use subsidiaries to limit liability | Medium |
| **Consent as escape valve** | Obtain consent to avoid requirements | Medium |

### Shortcomings

| Issue | Impact | Root Cause |
|-------|--------|------------|
| **Enforcement resources** | Inadequate capacity | Budget constraints |
| **Technical complexity** | Hard to verify compliance | Expertise gap |
| **Speed of technology** | Laws become outdated | Static rules |
| **International gaps** | Limited global reach | Sovereignty |
| **Interpretability limits** | Cannot fully explain AI | Technical reality |

### Rectification Procedures

1. **Functional definitions** capturing substance over form
2. **Adaptive mechanisms** allowing regular updates
3. **Extraterritorial reach** for systems affecting US persons
4. **Mandatory audits** by qualified third parties
5. **Piercing corporate structures** for liability
6. **Whistleblower incentives** for internal reporting
7. **International cooperation** provisions
8. **Technical expertise** investment in agencies
9. **Sunset and review** requirements
10. **Private enforcement** to supplement agency action

### General Implementation Concerns

| Concern | Mitigation |
|---------|------------|
| Industry capture | Civil society participation, independent oversight |
| Political interference | Independent agency, term protections |
| Over/under-regulation | Adaptive mechanisms, regular review |
| Compliance burden | Tiered requirements, small entity exemptions |
| Enforcement consistency | Clear guidance, binding regulations |

## References

### Statutory References

- National AI Initiative Act of 2020
- Executive Order 14110 (Oct 2023)
- FTC Act, 15 U.S.C. § 45
- Civil Rights Act of 1964
- Fair Credit Reporting Act

### Key Cases

- *Loomis v. Wisconsin*, 881 N.W.2d 749 (Wis. 2016) (COMPAS)
- *FTC v. Kochava* (ongoing, location data)
- Various EEOC enforcement actions

### International References

- EU AI Act, Regulation (EU) 2024/xxx
- OECD AI Principles (2019)
- G7 Hiroshima AI Process (2023)

### Academic References

- Raji et al., "Closing the AI Accountability Gap" (2020)
- Calo, "Artificial Intelligence Policy: A Primer and Roadmap" (2017)
- Hadfield, "Regulatory Markets for AI Safety" (2023)

## Related Topics

- [Algorithmic Accountability: Legislation](../algorithmic-accountability/11-legislation.md)
- [Data Privacy: Legislation](../data-privacy/11-legislation.md)
- [Surveillance Technology: Legislation](../surveillance-technology/11-legislation.md)
- [Big Tech Antitrust: Legislation](../big-tech-antitrust/11-legislation.md)

---

## Document Navigation

- Previous: [Actions](10-actions.md)
- Up: [Overview](01-overview.md)
