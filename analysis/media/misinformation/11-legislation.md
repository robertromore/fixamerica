# Misinformation: Legislation and Legal Framework

## Overview

Addressing misinformation through legislation requires navigating significant First Amendment constraints. Government cannot regulate the content of speech, but it can require transparency, fund research and education, and address the structural factors that enable misinformation to spread. This document provides draft legal text for constitutionally permissible interventions.

## Federal Legislation

### Platform Transparency for Misinformation Act

**Purpose**: Require platforms to disclose information about how misinformation spreads on their services.

**Draft Text**:

```text
SECTION 1. SHORT TITLE.

This Act may be cited as the "Platform Transparency for Misinformation
Act of 2025".

SEC. 2. DEFINITIONS.

(a) COVERED PLATFORM.—A "covered platform" means an online platform
that—
    (1) has more than 50 million monthly active users in the United
    States; or
    (2) has more than $1 billion in annual revenue.

(b) MISINFORMATION.—For purposes of this Act, "misinformation" means
verifiably false or misleading information about—
    (1) public health, including vaccine safety, disease transmission,
    or medical treatments;
    (2) election administration, including voting locations, times,
    or eligibility;
    (3) imminent threats to public safety; or
    (4) other categories as determined by the Commission through
    rulemaking.

(c) AMPLIFICATION.—"Amplification" means the use of algorithmic
systems to increase the distribution or visibility of content beyond
what would occur through chronological display or user-initiated
sharing.

SEC. 3. TRANSPARENCY REQUIREMENTS.

(a) QUARTERLY REPORTS.—Each covered platform shall publish quarterly
reports including—
    (1) the number of instances of content identified by fact-checkers
    as containing misinformation;
    (2) the amplification received by such content before and after
    identification;
    (3) actions taken in response to identified misinformation;
    (4) the reach of corrections compared to original content;
    (5) appeals and reversal rates; and
    (6) breakdown by category of misinformation.

(b) ALGORITHMIC IMPACT DISCLOSURE.—Each covered platform shall
disclose annually—
    (1) how algorithmic systems affect the spread of content identified
    as misinformation;
    (2) whether misinformation receives greater amplification than
    corrections;
    (3) changes made to algorithmic systems to reduce misinformation
    spread; and
    (4) effectiveness of such changes.

SEC. 4. RESEARCH ACCESS.

(a) REQUIREMENT.—Each covered platform shall provide qualified
researchers access to data necessary to study—
    (1) the spread of misinformation on the platform;
    (2) the effectiveness of interventions;
    (3) the role of algorithmic amplification; and
    (4) other topics related to information integrity.

(b) PRIVACY PROTECTIONS.—Access shall be subject to privacy
protections established by the Commission, including—
    (1) data minimization;
    (2) prohibition on re-identification;
    (3) secure data handling requirements; and
    (4) restrictions on data sharing.

(c) QUALIFIED RESEARCHERS.—The Commission shall establish criteria
for researcher qualification, which shall include—
    (1) affiliation with an accredited research institution;
    (2) institutional review board approval;
    (3) demonstrated research competence; and
    (4) commitment to publish findings publicly.

SEC. 5. ENFORCEMENT.

(a) FTC AUTHORITY.—The Federal Trade Commission shall enforce this
Act as an unfair or deceptive trade practice under Section 5 of the
FTC Act.

(b) PENALTIES.—
    (1) Violations shall be subject to civil penalties of up to
    $50,000 per day per violation.
    (2) Willful violations shall be subject to civil penalties of
    up to $100,000 per day per violation.

(c) STATE ENFORCEMENT.—Nothing in this Act shall preempt State
attorneys general from enforcing State consumer protection laws
related to platform transparency.

SEC. 6. RULEMAKING.

The Commission shall promulgate rules implementing this Act within
18 months of enactment, including—
    (1) specific disclosure requirements;
    (2) research access protocols;
    (3) privacy protections; and
    (4) enforcement procedures.

SEC. 7. LIMITATION.

Nothing in this Act shall be construed to—
    (1) require or authorize the government to determine the truth
    or falsity of any speech;
    (2) require platforms to remove or restrict any content; or
    (3) impose liability for the content of user speech.

SEC. 8. EFFECTIVE DATE.

This Act shall take effect 180 days after enactment.
```

**Explanation**:

- Focuses on transparency, not content regulation
- Requires disclosure about misinformation spread, not removal
- Enables research without mandating platform action
- Preserves First Amendment protections
- Provides meaningful enforcement mechanism

### Media Literacy and Critical Thinking Act

**Purpose**: Fund media literacy education to help citizens evaluate information.

**Draft Text**:

```text
SECTION 1. SHORT TITLE.

This Act may be cited as the "Media Literacy and Critical Thinking
Act of 2025".

SEC. 2. FINDINGS.

Congress finds that—
    (1) misinformation poses significant threats to public health,
    democratic participation, and social cohesion;
    (2) media literacy education is an effective, speech-protective
    approach to building resilience against misinformation;
    (3) other democracies, particularly Finland, have demonstrated
    the effectiveness of comprehensive media literacy education;
    (4) current media literacy education in the United States is
    inconsistent and inadequate; and
    (5) federal support can help states develop and implement
    effective programs.

SEC. 3. DEFINITIONS.

(a) MEDIA LITERACY.—"Media literacy" means the ability to—
    (1) access, analyze, evaluate, create, and act using all forms
    of communication;
    (2) understand the role of media in society;
    (3) apply critical thinking skills to media messages; and
    (4) recognize and resist manipulative communication techniques.

(b) INFORMATION LITERACY.—"Information literacy" means the ability
to—
    (1) recognize when information is needed;
    (2) locate, evaluate, and effectively use information; and
    (3) understand the economic, legal, and social issues surrounding
    the use of information.

SEC. 4. GRANT PROGRAM.

(a) ESTABLISHMENT.—The Secretary of Education shall establish a
grant program to support State and local media literacy education
initiatives.

(b) ELIGIBLE USES.—Grant funds may be used for—
    (1) curriculum development aligned with State standards;
    (2) teacher professional development;
    (3) instructional materials;
    (4) after-school and summer programs;
    (5) community education programs through libraries and community
    organizations;
    (6) programs targeting seniors and other vulnerable populations;
    (7) evaluation and research; and
    (8) coordination among stakeholders.

(c) PRIORITY.—In awarding grants, the Secretary shall give priority
to applications that—
    (1) integrate media literacy across subject areas;
    (2) include evidence-based curricula;
    (3) provide for program evaluation;
    (4) demonstrate sustainability beyond grant period; and
    (5) include partnerships with community organizations.

(d) AUTHORIZATION.—There is authorized to be appropriated $200
million annually for fiscal years 2025 through 2030.

SEC. 5. NATIONAL CLEARINGHOUSE.

(a) ESTABLISHMENT.—The Secretary shall establish a national
clearinghouse for media literacy education resources, which shall—
    (1) compile evidence-based curricula and materials;
    (2) disseminate best practices;
    (3) provide technical assistance to States and localities;
    (4) coordinate research and evaluation; and
    (5) facilitate sharing among practitioners.

(b) AUTHORIZATION.—There is authorized to be appropriated $10
million annually for the clearinghouse.

SEC. 6. RESEARCH.

(a) STUDY.—The Secretary, in coordination with the National Science
Foundation, shall conduct or support research on—
    (1) the effectiveness of media literacy interventions;
    (2) best practices for different populations;
    (3) long-term impacts of media literacy education; and
    (4) adaptation of international models.

(b) AUTHORIZATION.—There is authorized to be appropriated $25
million annually for research under this section.

SEC. 7. REPORTS.

The Secretary shall report to Congress annually on—
    (1) grants awarded and activities supported;
    (2) research findings;
    (3) state of media literacy education nationally; and
    (4) recommendations for improvement.

SEC. 8. EFFECTIVE DATE.

This Act shall take effect upon enactment.
```

**Explanation**:

- Education-focused approach respects First Amendment
- Builds on successful international models
- Provides resources without mandating curriculum
- Includes research component for evidence-based improvement

### Prebunking and Inoculation Research Act

**Purpose**: Fund research on psychological inoculation against misinformation.

**Draft Text**:

```text
SECTION 1. SHORT TITLE.

This Act may be cited as the "Prebunking and Inoculation Research
Act of 2025".

SEC. 2. FINDINGS.

Congress finds that—
    (1) psychological "inoculation" against misinformation has shown
    promise in research studies;
    (2) prebunking—exposing people to weakened forms of misinformation
    before they encounter it—can build resistance;
    (3) this approach is speech-protective because it empowers
    individuals rather than restricting content;
    (4) additional research is needed to develop scalable interventions;
    and
    (5) public health and election integrity could benefit from
    effective prebunking programs.

SEC. 3. NSF RESEARCH PROGRAM.

(a) ESTABLISHMENT.—The Director of the National Science Foundation
shall establish a research program on misinformation inoculation
and prebunking.

(b) RESEARCH AREAS.—The program shall support research on—
    (1) psychological mechanisms of inoculation;
    (2) effective prebunking messages and techniques;
    (3) scalable delivery methods;
    (4) effectiveness across different populations;
    (5) duration of protective effects; and
    (6) integration with media literacy education.

(c) AUTHORIZATION.—There is authorized to be appropriated $50
million annually for fiscal years 2025 through 2030.

SEC. 4. PILOT PROGRAMS.

(a) HEALTH MISINFORMATION.—The Secretary of Health and Human Services,
in coordination with the Director of the Centers for Disease Control
and Prevention, shall develop and pilot prebunking campaigns for
health misinformation.

(b) ELECTION MISINFORMATION.—The Director of the Cybersecurity and
Infrastructure Security Agency shall develop and pilot prebunking
programs for election misinformation, to be deployed before major
elections.

(c) EVALUATION.—All pilot programs shall include rigorous evaluation
components.

(d) AUTHORIZATION.—There is authorized to be appropriated $30
million annually for pilot programs.

SEC. 5. COORDINATION.

The Director of the Office of Science and Technology Policy shall
coordinate prebunking research and programs across federal agencies.

SEC. 6. INTERNATIONAL COOPERATION.

The Secretary of State shall work with international partners to—
    (1) share research findings;
    (2) coordinate on cross-border misinformation; and
    (3) develop best practices.

SEC. 7. EFFECTIVE DATE.

This Act shall take effect upon enactment.
```

### Foreign Disinformation Response Enhancement Act

**Purpose**: Strengthen government capacity to identify and counter foreign disinformation.

**Draft Text**:

```text
SECTION 1. SHORT TITLE.

This Act may be cited as the "Foreign Disinformation Response
Enhancement Act of 2025".

SEC. 2. FINDINGS.

Congress finds that—
    (1) foreign adversaries, including Russia, China, and Iran,
    conduct disinformation operations targeting the United States;
    (2) such operations threaten national security, electoral
    integrity, and social cohesion;
    (3) current government response capabilities are fragmented
    and inadequate; and
    (4) enhanced coordination and resources are needed.

SEC. 3. INTERAGENCY COORDINATION.

(a) TASK FORCE.—The President shall establish an Interagency Task
Force on Foreign Disinformation, which shall include representatives
from—
    (1) the Office of the Director of National Intelligence;
    (2) the Department of State;
    (3) the Department of Homeland Security;
    (4) the Federal Bureau of Investigation;
    (5) the Department of Defense;
    (6) the Federal Trade Commission; and
    (7) such other agencies as the President determines appropriate.

(b) RESPONSIBILITIES.—The Task Force shall—
    (1) coordinate detection of foreign disinformation operations;
    (2) develop response strategies;
    (3) coordinate with allied governments;
    (4) work with platforms on detection and response;
    (5) inform the public about foreign disinformation threats; and
    (6) develop recommendations for legislative action.

SEC. 4. STATE DEPARTMENT CAPACITY.

(a) GLOBAL ENGAGEMENT CENTER.—The Global Engagement Center shall
be expanded and authorized through 2030.

(b) AUTHORIZATION.—There is authorized to be appropriated $100
million annually for the Global Engagement Center.

SEC. 5. PUBLIC REPORTING.

(a) ANNUAL REPORT.—The Director of National Intelligence shall
publish an annual unclassified report on foreign disinformation
threats to the United States, including—
    (1) identification of foreign actors;
    (2) description of campaigns detected;
    (3) assessment of impacts; and
    (4) recommendations for response.

(b) ELECTION-SPECIFIC REPORTING.—Before each federal election, the
Director shall publish a report on foreign disinformation threats
to election integrity.

SEC. 6. PLATFORM COOPERATION.

(a) INFORMATION SHARING.—The Task Force shall establish a mechanism
for sharing information about foreign disinformation operations with
covered platforms.

(b) VOLUNTARY COOPERATION.—Nothing in this Act shall require platforms
to take specific actions in response to shared information.

SEC. 7. CIVIL LIBERTIES PROTECTIONS.

(a) LIMITATION.—Nothing in this Act shall authorize the government
to—
    (1) monitor or collect information about domestic speech;
    (2) require platforms to remove content; or
    (3) take any action inconsistent with the First Amendment.

(b) OVERSIGHT.—The Privacy and Civil Liberties Oversight Board shall
review Task Force activities annually.

SEC. 8. EFFECTIVE DATE.

This Act shall take effect upon enactment.
```

## State Model Legislation

### Model State Media Literacy Education Act

```text
SECTION 1. SHORT TITLE.

This Act may be cited as the "[State] Media Literacy Education Act".

SECTION 2. DEFINITIONS.

(a) "Media literacy" means the ability to access, analyze, evaluate,
create, and act using all forms of communication.

(b) "Information literacy" means the ability to recognize information
needs, locate and evaluate information, and use it effectively.

SECTION 3. CURRICULUM STANDARDS.

(a) The [State Board of Education] shall develop or adopt media
literacy and information literacy standards for grades K-12.

(b) Standards shall address—
    (1) evaluating the credibility of sources;
    (2) recognizing bias and propaganda techniques;
    (3) understanding how algorithms affect information exposure;
    (4) responsible creation and sharing of content; and
    (5) rights and responsibilities in digital environments.

SECTION 4. IMPLEMENTATION.

(a) Each school district shall integrate media literacy instruction
into existing curriculum within 3 years of this Act's effective date.

(b) The [Department of Education] shall provide—
    (1) model curricula and instructional materials;
    (2) professional development for teachers; and
    (3) technical assistance to districts.

SECTION 5. TEACHER PREPARATION.

Teacher preparation programs at state institutions shall include
instruction in media literacy pedagogy.

SECTION 6. COMMUNITY EDUCATION.

The [State Library] shall develop media literacy programs for adults,
with priority for seniors and other populations vulnerable to
misinformation.

SECTION 7. FUNDING.

[Funding mechanism appropriate to state]

SECTION 8. EFFECTIVE DATE.

This Act takes effect [date].
```

### Model State Election Misinformation Response Act

```text
SECTION 1. SHORT TITLE.

This Act may be cited as the "[State] Election Misinformation
Response Act".

SECTION 2. FINDINGS.

The Legislature finds that—
    (1) misinformation about election administration undermines
    democratic participation;
    (2) voters need accurate information about voting locations,
    times, and procedures; and
    (3) the state has an interest in ensuring voters can exercise
    their rights.

SECTION 3. RAPID RESPONSE SYSTEM.

(a) The [Secretary of State] shall establish a rapid response system
for election misinformation.

(b) The system shall—
    (1) monitor for false information about election administration;
    (2) quickly publish corrections through official channels;
    (3) coordinate with local election officials; and
    (4) work with media and platforms on corrections.

SECTION 4. OFFICIAL INFORMATION.

(a) The [Secretary of State] shall maintain authoritative, accessible
information about—
    (1) voter registration procedures;
    (2) polling locations and hours;
    (3) ballot drop-off locations;
    (4) vote-by-mail procedures; and
    (5) election security measures.

(b) This information shall be available in multiple languages and
accessible formats.

SECTION 5. PREBUNKING.

Before each election, the [Secretary of State] shall conduct public
education campaigns to prebunk common election misinformation.

SECTION 6. CIVIL PENALTIES.

Any person who knowingly disseminates false information about the
time, place, or manner of elections with the intent to prevent
eligible voters from voting shall be subject to civil penalties
of up to [$X] per violation.

SECTION 7. LIMITATION.

Nothing in this Act shall—
    (1) restrict political speech or opinion;
    (2) apply to good-faith errors; or
    (3) restrict media reporting.

SECTION 8. EFFECTIVE DATE.

This Act takes effect [date].
```

## Constitutional Considerations

### First Amendment Analysis

| Approach | Constitutional Status |
|----------|----------------------|
| **Transparency requirements** | Likely permissible (commercial speech) |
| **Research access mandates** | Likely permissible (structural regulation) |
| **Content removal mandates** | Likely unconstitutional |
| **Government truth determinations** | Unconstitutional |
| **Foreign disinfo response** | Permissible (national security) |
| **Election admin disinfo** | Narrowly permissible (voting rights) |
| **Education funding** | Clearly permissible |

### Key Principles

1. **Government cannot be arbiter of truth** for protected speech
2. **Structural regulation** (transparency, research access) more likely to survive
3. **Content-neutral rules** preferred over content-based
4. **Narrow targeting** of specific harms more defensible
5. **Private platform action** not state action (generally)

## Loopholes, Shortcomings, and Rectification

### Potential Loopholes

| Loophole | Description | Severity |
|----------|-------------|----------|
| **Definition gaming** | Restructure to avoid "covered platform" | High |
| **Minimal compliance** | Technically comply without meaningful disclosure | High |
| **Research obstruction** | Make access technically difficult | Medium |
| **Jurisdiction avoidance** | Operate from outside US | Medium |
| **"Fact-check" gaming** | Create fake fact-checkers | Medium |

### Shortcomings

| Issue | Impact | Root Cause |
|-------|--------|------------|
| **First Amendment constraints** | Limits direct content regulation | Constitutional |
| **Speed mismatch** | Regulation slower than misinfo | Structural |
| **Platform resistance** | Lobbying, litigation | Political economy |
| **Cross-border content** | Jurisdiction limits | Sovereignty |
| **Definitional challenges** | What counts as "misinformation" | Inherent complexity |

### Rectification Procedures

1. **Functional definitions** that capture new platforms and structures
2. **Meaningful disclosure standards** with specific requirements, not vague language
3. **Research access with teeth** including penalties for obstruction
4. **Regular review** mechanisms built into legislation
5. **State enforcement** to supplement federal capacity
6. **Private rights of action** for researcher access violations
7. **International coordination** for cross-border content
8. **Adaptive authority** for agencies to update rules

### General Implementation Concerns

| Concern | Mitigation |
|---------|------------|
| **Agency capture** | Multi-agency oversight, transparency |
| **Political weaponization** | Non-partisan criteria, judicial review |
| **Unintended chilling** | Clear safe harbors, appeals processes |
| **Technology evolution** | Flexible, principles-based rules |
| **Resource constraints** | Adequate appropriations |

## References

### Constitutional Law

- *United States v. Alvarez*, 567 U.S. 709 (2012) (Stolen Valor Act; false statements)
- *Reed v. Town of Gilbert*, 576 U.S. 155 (2015) (content-neutral regulation)
- *Packingham v. North Carolina*, 137 S. Ct. 1730 (2017) (social media access)

### Statutory References

- 47 U.S.C. § 230 (Section 230)
- 52 U.S.C. § 20511 (Election crimes)
- 18 U.S.C. § 1038 (False information and hoaxes)

### International References

- EU Digital Services Act, Regulation 2022/2065
- EU Code of Practice on Disinformation
- UK Online Safety Act 2023

### Academic Sources

- Benkler, Y., Faris, R., & Roberts, H. (2018). *Network Propaganda*
- Guess, A., et al. (2019). "Less than you think" (misinformation exposure)
- Roozenbeek, J., & van der Linden, S. (2019). "Fake news game" (inoculation)

## Related Topics

- [Platform Power: Legislation](../platform-power/11-legislation.md)
- [Press Freedom: Legislation](../press-freedom/11-legislation.md)
- [Public Media: Legislation](../public-media/11-legislation.md)

---

## Document Navigation

- Previous: [Actions](10-actions.md)
- Up: [Overview](01-overview.md)
