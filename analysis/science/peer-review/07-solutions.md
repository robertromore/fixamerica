# Peer Review: Solutions

## Overview

Solutions to peer review's problems must address both journal publication review and federal grant review, while navigating strong institutional inertia and legitimate concerns about unintended consequences. The most promising reforms work with, rather than against, existing incentives and can be adopted incrementally.

## Journal Peer Review Reforms

### Solution 1: Open Peer Review with Published Reports

**Problem addressed**: Reviewer anonymity enables bias, laziness, and hostility without accountability.

**How it works**:

- Reviewer identities disclosed to authors (and optionally to readers)
- Full review reports published alongside accepted papers
- Authors may publish responses to reviews
- Reviewers receive public credit for their work

**Evidence**:

| Study | Finding |
|-------|---------|
| Walsh et al. (2000), *British Journal of Psychiatry* | Signed reviews showed no decline in quality |
| *BMJ* experience (1999-present) | Open review maintained rigor; reviewer recruitment stable |
| *eLife* model (2023-present) | Published reviews with assessments received positively |
| van Rooyen et al. (1999) | Reviewer quality ratings unaffected by opening identity |

**Implementation models**:

| Model | Openness Level | Adopters |
|-------|---------------|----------|
| Signed reviews (identity known to author) | Medium | *BMJ*, *EMBO Journal* |
| Published reports (identity optional) | Medium-High | *Nature Communications*, *eLife* |
| Full transparency (identity + reports public) | High | *F1000Research*, *Peer Community In* |
| Hybrid (reviewer chooses) | Flexible | *PeerJ*, *Royal Society Open Science* |

**Recommendation**: Implement graduated openness. Begin with published anonymous reports, then offer opt-in reviewer identification, building toward default transparency over 5 years.

### Solution 2: Preprint-Plus-Review Models

**Problem addressed**: Publication delays of 6-18 months slow knowledge dissemination; cascading rejections waste reviewer effort.

**How it works**:

- Authors post preprints immediately upon completion
- Overlay review services evaluate preprints and attach formal reviews
- Journals curate and endorse reviewed preprints rather than controlling first access
- Review is decoupled from publication venue

**Existing models**:

| Platform | Model | Status |
|----------|-------|--------|
| arXiv + overlay journals | Preprint server + volunteer review | Established in mathematics, physics |
| *Peer Community In* (PCI) | Free, community-led overlay review | Growing; 15+ communities |
| *eLife* reviewed preprints | Reviews published on preprints | Implemented 2023 |
| *Review Commons* | Portable reviews across journals | Launched 2019; supported by EMBO |

**Benefits**:

- Eliminates months-to-years of publication delay
- Reduces redundant re-reviewing when manuscripts move between journals
- Establishes priority immediately
- Enables rapid response to urgent findings (pandemics, public health crises)
- Review effort is preserved regardless of journal outcome

**Recommendation**: Federal agencies should require preprint posting for all funded research. Journal subscription decisions should recognize preprint+review as equivalent to traditional publication for career evaluation.

### Solution 3: Registered Reports

**Problem addressed**: Publication bias and questionable research practices driven by results-dependent review.

**How it works**:

1. Authors submit study design and analysis plan *before* conducting research
2. Peer review evaluates the question, design, and methods
3. If approved, the journal commits to publish regardless of results
4. After data collection, a second review verifies adherence to protocol
5. Publication follows with "in-principle acceptance"

**Evidence**:

- Over 300 journals now offer registered reports
- Studies show no decline in citation rates vs. traditional articles (Soderberg et al., 2021)
- Registered reports produce more null results (35% vs. 5% in traditional articles), reducing publication bias
- Eliminates p-hacking, HARKing, and other questionable research practices

**Recommendation**: All federally funded clinical trials and replication studies should use registered report formats. Journals should be encouraged to adopt registered reports as an option across all disciplines.

### Solution 4: Reviewer Compensation and Credit

**Problem addressed**: Reviewers perform essential unpaid labor, leading to burnout, declining participation, and poor review quality.

**How it works**:

| Approach | Mechanism | Cost |
|----------|-----------|------|
| Direct payment | $200-500 per review | ~$3-7.5 billion/year globally |
| Journal credit | Free submission, publication fee waiver, or subscription access | Lower direct cost |
| Career credit | Reviews count toward tenure, promotion, and grant applications | Systemic change needed |
| Public recognition | Published reviewer profiles, badges, metrics | Minimal cost |
| Tax deduction | Treat review hours as charitable contribution to science | Revenue cost to government |

**Current recognition platforms**:

- Publons/Web of Science Reviewer Recognition
- ReviewerCredits
- ORCID integration for review activity
- Peerage of Science

**Recommendation**: Implement a three-tier approach: (1) immediate public recognition through standardized review credit, (2) career credit by mandating that tenure committees consider review service, (3) phase in direct compensation funded by Article Processing Charges or institutional subscriptions.

### Solution 5: AI-Assisted Review Tools

**Problem addressed**: Reviewer overload, inconsistent quality checks, and difficulty matching reviewers to manuscripts.

**How it works**:

- AI tools perform initial screening for methodological rigor, statistical errors, and reproducibility criteria
- Automated reviewer matching using expertise profiles and conflict-of-interest databases
- AI-generated review templates highlight areas requiring human expert attention
- Plagiarism and image manipulation detection

**Current tools**:

| Tool | Function | Status |
|------|----------|--------|
| StatReviewer | Statistical methodology checks | Deployed at ~20 journals |
| SciScore | Rigor and reproducibility criteria | Growing adoption |
| ITHENTICATE | Plagiarism detection | Deployed at ~90% of major journals |
| Proofig | Image duplication detection | Emerging |
| Semantic Scholar SPECTER | Reviewer-paper matching | Research prototype |

**Limitations and risks**:

- AI cannot replace expert judgment on scientific significance or novelty
- Risk of reviewers submitting AI-generated reviews without genuine engagement
- Bias in AI training data could replicate existing biases
- Over-reliance on automated checks could create false sense of rigor

**Recommendation**: Deploy AI as reviewer *support* tools, not replacements. Require disclosure of AI assistance in reviews. Invest in tools that reduce reviewer burden on mechanical checks while preserving human judgment for scientific substance.

## Grant Peer Review Reforms

### Solution 6: Partial Randomization (Modified Lottery)

**Problem addressed**: Low reliability of grant scores; conservative bias; enormous time spent on unfunded proposals.

**How it works**:

1. Expert panels triage proposals into three categories: fund, reject, or "fundable"
2. The "fundable" category (proposals above a quality threshold but not clearly top-ranked) enters a lottery
3. Random selection among fundable proposals replaces fine-grained scoring

**Evidence**:

- Pier et al. (2018) showed NIH scores are unreliable for proposals between 10th-30th percentile
- The Swiss National Science Foundation piloted partial randomization with positive results
- New Zealand Health Research Council implemented a modified lottery in 2015
- Simulations show random allocation among qualified proposals produces equivalent research quality

**Benefits**:

- Reduces time spent on detailed scoring that is functionally random anyway
- Decreases conservative bias (novel proposals less likely penalized)
- Reduces demographic bias (random selection cannot discriminate)
- Frees reviewer time for more meaningful evaluation
- Reduces the psychological burden of false precision in scoring

**Recommendation**: NIH and NSF should pilot partial randomization for R21 (exploratory) grants and small awards, expanding to R01 if results are positive.

### Solution 7: Anonymous Grant Review

**Problem addressed**: Halo effect from investigator prestige and institutional affiliation biases scoring.

**How it works**:

- Remove investigator biographical information from initial review
- Evaluate research plan on its scientific merits alone
- Reveal investigator qualifications only after initial scoring for feasibility assessment
- Blind institutional affiliation and prior funding history

**Evidence**:

- NIH piloted anonymous review for R01 applications (2022-2024)
- Preliminary results suggest reduced prestige bias and modestly improved diversity of funded investigators
- Austrian Science Fund (FWF) implemented anonymized review with positive equity outcomes
- The Gates Foundation uses blinded review for some grant programs

**Recommendation**: NIH should expand anonymous review pilots to all R01 applications, with a phased implementation over 3 years.

### Solution 8: Streamlined Grant Applications

**Problem addressed**: Excessive application burden wastes researcher time on unfunded proposals ($15+ billion annually in estimated lost productivity).

**How it works**:

| Current NIH R01 | Proposed Streamlined Version |
|-----------------|------------------------------|
| 12-page research strategy | 5-page research strategy |
| Detailed budget justification | Modular budget (standard amounts) |
| Biosketches for all key personnel | PI biosketch only (anonymized) |
| Letters of support | Eliminated |
| Facilities description | Standardized institutional profile |
| Preliminary data expected | Not required for initial submission |

**Evidence**:

- NSF RAPID awards use streamlined 5-page proposals with high success and impact
- Canadian Institutes of Health Research shortened applications with no decline in funded research quality
- The Wellcome Trust's streamlined application attracted a more diverse applicant pool

**Recommendation**: Reduce R01 applications to 5 pages for initial review. Use a two-stage process: short proposal for initial triage, then detailed plan only for top-scoring applications.

### Solution 9: Diversified Review Panels

**Problem addressed**: Demographic homogeneity of review panels perpetuates bias in funding allocation.

**How it works**:

- Mandate demographic diversity targets for study section composition
- Recruit reviewers from primarily undergraduate institutions, MSIs, and community colleges
- Include patient and community representatives on translational research panels
- Provide mentored reviewer training programs for underrepresented groups
- Compensate reviewers adequately to enable participation by those without institutional support

**Current NIH initiatives**:

- Early Career Reviewer (ECR) program: expanded from ~150 to ~400 reviewers per round
- Implicit bias training: mandatory for all study section members since 2022
- Diversity supplements: encourage diverse researcher participation
- FIRST (Faculty Institutional Recruitment for Sustainable Transformation): institutional hiring cohorts

**Recommendation**: Set explicit diversity benchmarks for study sections. Track and publish demographic composition of review panels and correlations with funding outcomes.

## Cross-Cutting Solutions

### Solution 10: Reform Tenure and Promotion Criteria

**Problem addressed**: The root driver of publish-or-perish culture and impact factor obsession.

**How it works**:

- Universities adopt DORA principles: evaluate research on its own merits, not journal prestige
- Tenure dossiers include narrative research impact statements, not just publication lists
- Reviewer service, mentoring, and data sharing count toward tenure
- Preprints and non-traditional outputs (datasets, software, protocols) receive formal credit

**Progress**:

| Country/Institution | Action | Status |
|---------------------|--------|--------|
| Netherlands | All universities agreed to abandon metrics-based evaluation | 2019 |
| UK | Research Excellence Framework (REF) evaluates outputs, not venues | Ongoing |
| University of Ghent | Adopted narrative CV for hiring and promotion | 2019 |
| MIT | Faculty handbook revised to value diverse scholarly contributions | 2023 |
| DORA signatories | 3,000+ organizations pledged | Growing |

**Recommendation**: Federal agencies should require DORA-aligned evaluation in institutional grant applications. Accreditation bodies should incorporate research assessment reform into institutional standards.

### Solution 11: Portable Peer Review

**Problem addressed**: Cascading rejections waste reviewer effort; the same paper reviewed repeatedly across journals.

**How it works**:

- Reviews from one journal are shared with subsequent journals if the paper is rejected
- Authors control whether to share reviews with new journals
- Journals accept transferred reviews to reduce re-reviewing burden
- Standardized review formats enable cross-journal portability

**Existing platforms**:

| Platform | How It Works | Journals Participating |
|----------|-------------|----------------------|
| Review Commons | Journal-independent review; reviews transfer to 17+ affiliated journals | *EMBO* family, *eLife*, others |
| Peerage of Science | Portable reviews across participating journals | ~20 journals |
| Neuroscience Peer Review Consortium | Reviews transfer among 70+ neuroscience journals | 70+ |

**Recommendation**: Federal funders should encourage journal adoption of portable review. Professional societies should establish discipline-wide review transfer agreements.

## Solution Summary Matrix

| Solution | Impact | Feasibility | Timeline | Authority Needed |
|----------|--------|-------------|----------|-----------------|
| Open peer review | High | Medium | 3-5 years | Journal/publisher |
| Preprint-plus-review | Very High | Medium | 3-7 years | Funder mandate + culture change |
| Registered reports | High | Medium-High | 2-5 years | Journal adoption |
| Reviewer compensation | Medium-High | Medium | 3-5 years | Publisher/funder |
| AI-assisted review | Medium | High | 1-3 years | Publisher/journal |
| Partial randomization | High | Medium | 3-5 years | Funding agency |
| Anonymous grant review | High | Medium-High | 2-4 years | Funding agency |
| Streamlined applications | Medium-High | High | 1-3 years | Funding agency |
| Diversified panels | High | Medium | 2-5 years | Funding agency |
| Tenure reform | Very High | Low | 5-10 years | Universities/accreditors |
| Portable review | Medium | Medium-High | 2-4 years | Journal consortium |

## Sources

- Soderberg, Courtney K., et al. "Initial Evidence of Research Quality of Registered Reports Compared with the Standard Publishing Model." *Nature Human Behaviour* 5, no. 8 (2021): 990-997.
- Pier, Elizabeth L., et al. "Low Agreement Among Reviewers Evaluating the Same NIH Grant Applications." *PNAS* 115, no. 12 (2018): 2952-2957.
- Gross, Kevin, and Carl T. Bergstrom. "Contest Models Highlight Inherent Inefficiencies of Scientific Funding Competitions." *PLoS Biology* 17, no. 1 (2019): e3000065.
- Ross-Hellauer, Tony. "What Is Open Peer Review? A Systematic Review." *F1000Research* 6 (2017): 588.
- Fang, Ferric C., and Arturo Casadevall. "Competitive Science: Is Competition Ruining Science?" *Infection and Immunity* 83, no. 4 (2015): 1229-1233.
- DORA. "San Francisco Declaration on Research Assessment." <https://sfdora.org/read/>
- Review Commons. "About Review Commons." <https://www.reviewcommons.org/about/>

## Document Navigation

- Previous: [Opposition](06-opposition.md)
- Next: [Roadmap](08-roadmap.md)
- Up: [Science](../01-overview.md)
