# Peer Review: Current State

## The Problem Today

### A System Under Strain

The peer review system in 2025 faces a convergence of pressures that threaten its ability to serve as science's quality-control mechanism. The volume of scientific output has grown exponentially--global publications exceed 3 million annually--while the pool of qualified reviewers has not kept pace. Journals report declining acceptance rates for review invitations, with some editors needing to contact 10 or more potential reviewers to secure two reports. Meanwhile, the system's well-documented biases remain largely unaddressed, and the rise of preprint culture has begun to challenge whether traditional pre-publication review is even necessary.

### Journal Peer Review

#### Volume and Capacity

| Metric | 2010 | 2020 | 2025 (est.) |
|--------|------|------|-------------|
| Articles published globally | ~1.8 million | ~2.8 million | ~3.2 million |
| Active journals (peer-reviewed) | ~28,000 | ~33,000 | ~36,000 |
| Reviewer invitations per article | ~5 | ~7 | ~8-10 |
| Invitation acceptance rate | ~55% | ~40% | ~30-35% |
| Average time to first decision | 3-4 months | 4-5 months | 4-6 months |

#### The Desk Rejection Bottleneck

Top-tier journals desk-reject (reject without external review) 50-80% of submissions. This creates a cascade effect:

- Rejected authors resubmit to lower-tier journals
- The same manuscript may be reviewed multiple times across different journals
- Estimated 15 million redundant review hours annually
- Average paper is reviewed at 2-3 journals before acceptance

#### Current Review Models in Practice

| Model | Adoption Rate | Notable Adopters |
|-------|---------------|-----------------|
| Single-blind | ~55% of journals | Most traditional journals |
| Double-blind | ~30% of journals | Social sciences, humanities |
| Open review | ~10% of journals | *BMJ*, *eLife*, *F1000Research* |
| Post-publication | ~3% of journals | *F1000Research*, overlay journals |
| Registered reports | ~300+ journals | Psychology, neuroscience leaders |

### Grant Peer Review

#### NIH Study Sections

The National Institutes of Health operates the largest federally funded biomedical research review system in the world. As of 2025:

**Structure**

- ~180 chartered (standing) study sections organized under the Center for Scientific Review (CSR)
- ~200 Special Emphasis Panels (SEPs) convened per cycle for overflow and special topics
- Each study section has ~20-30 members serving 4-year terms
- Approximately 18,000 reviewers participate per cycle

**Current Performance**

| Metric | Value | Context |
|--------|-------|---------|
| R01 applications per cycle | ~30,000 | Each reviewed by 2-3 assigned reviewers |
| R01 success rate | ~20% | Down from ~30% in 2000 |
| Average percentile for funding | ~20th | Varies by institute; some fund to 10th |
| Average R01 applicant age (first award) | 43 | Up from 36 in 1980 |
| Time from submission to decision | ~9 months | Three review cycles per year |
| Estimated researcher time per unfunded R01 | 200-500 hours | ~$15 billion in aggregate lost productivity |

**Scoring Criteria**

| Criterion | Weight | Description |
|-----------|--------|-------------|
| Significance | Equal | Does the project address an important problem? |
| Investigator(s) | Equal | Are investigators qualified and appropriate? |
| Innovation | Equal | Does the project employ novel concepts or approaches? |
| Approach | Equal | Is the strategy well-reasoned and feasible? |
| Environment | Equal | Is the scientific environment adequate? |

**Known Problems**

- *Conservatism bias*: Study sections prefer incremental, low-risk research over transformative proposals
- *Halo effect*: Investigator prestige disproportionately influences scores
- *Demographic skew*: Reviewers are disproportionately male, white, and from R1 institutions
- *Reliability concerns*: The same proposal scored by different panels can receive widely divergent scores

#### NSF Panel Review

The National Science Foundation uses a different model:

**Structure**

- Ad hoc panels assembled per program and competition
- Typically 3-6 panelists per proposal
- Program officers exercise significant discretion
- ~50,000 proposals reviewed annually

**Current Performance**

| Metric | Value |
|--------|-------|
| Overall success rate | ~25% |
| Average panel size | 10-15 reviewers |
| Number of external reviewers per proposal | 3-5 (mail reviews + panel) |
| Program officer discretion | Significant; can override panel rankings |

**Key Differences from NIH**

| Feature | NIH | NSF |
|---------|-----|-----|
| Panel type | Standing (chartered) | Ad hoc |
| Scoring | 1-9 numeric + percentile | Excellent/Very Good/Good/Fair/Poor |
| Officer discretion | Low (payline-driven) | High |
| Review criteria | 5 scored criteria | 2 criteria (merit + broader impacts) |
| Transparency | Summary statements provided | Panel summaries provided |

## Key Statistics

### Bias Documentation

#### Gender Bias

| Study | Finding | Year |
|-------|---------|------|
| Budden et al., *Trends in Ecology & Evolution* | Double-blind review increased female first-author acceptance by 7.9% | 2008 |
| Tomkins et al., *PNAS* | Single-blind reviewers scored male-authored papers higher than identical female-authored papers | 2017 |
| Squazzoni et al., *Science Advances* | Gender gaps in review outcomes persist across most disciplines | 2021 |
| Fox and Paine, *Functional Ecology* | Women's manuscripts held to higher revision standards | 2019 |

**NIH Grant Bias**

- Black PIs receive NIH R01 awards at approximately 55% the rate of white PIs after controlling for qualifications (Ginther et al., *Science*, 2011; updated 2023 data confirms persistence)
- Women PIs receive smaller average awards and lower success rates for R01 grants
- Asian-American PIs face a "model minority" penalty with lower discussion rates

#### Institutional Prestige Bias

| Finding | Source |
|---------|--------|
| Papers from top-20 universities 2-3x more likely to pass review | *Scientometrics* (2020) |
| Reviewer ratings correlate with author institution rank, not content quality | *Research Policy* (2019) |
| NIH R01 success rates 2x higher for top-10 vs. lower-ranked institutions | NIH Data Book (2024) |
| Removing institutional identifiers improves review fairness | *Nature Human Behaviour* (2022) |

#### Geographic Bias

| Finding | Source |
|---------|--------|
| US/UK reviewers systematically rate non-English-speaking-country papers lower | *Scientometrics* (2021) |
| African and South Asian submissions face 30-40% higher rejection rates | *PNAS* (2023) |
| Reviewer pools heavily skewed toward North America and Western Europe | *PLoS ONE* (2022) |

#### Methodological and Confirmation Bias

- Reviewers favor papers using methods similar to their own (Mahoney, *Cognitive Therapy and Research*, 1977; replicated 2019)
- Positive results 3-4x more likely to be published than null results (publication bias)
- Reviewers rate papers with statistically significant results more favorably regardless of methodological quality (Emerson et al., *JAMA*, 2010)

### Reviewer Burden

| Metric | Value |
|--------|-------|
| Average reviews completed per researcher per year | 4-8 |
| Estimated hours per review (journal) | 5-10 hours |
| Estimated hours per review (grant) | 10-20 hours |
| Compensation for journal reviews | $0 (industry standard) |
| Compensation for NIH study section service | ~$250/day + travel |
| Reviewers reporting burnout or overload | ~70% (survey data) |

### Publication Delays

| Field | Average Submission-to-Publication | Impact |
|-------|----------------------------------|--------|
| Biomedical sciences | 8-14 months | Clinical relevance lost |
| Physics (journal route) | 6-12 months | Preprints mitigate somewhat |
| Social sciences | 12-24 months | Policy relevance diminished |
| Humanities | 18-36 months | Career advancement delayed |

### The Preprint Disruption

| Platform | Established | Monthly Submissions (2025 est.) | Growth Rate |
|----------|-------------|--------------------------------|-------------|
| arXiv | 1991 | ~18,000 | ~10%/year |
| bioRxiv | 2013 | ~4,000 | ~20%/year |
| medRxiv | 2019 | ~2,500 | ~25%/year |
| SSRN | 1994 | ~8,000 | ~5%/year |
| ChemRxiv | 2017 | ~500 | ~15%/year |

**COVID-19 Acceleration**: The pandemic dramatically accelerated preprint adoption in biomedical sciences, with medRxiv and bioRxiv seeing 400%+ increases in 2020-2021. Many COVID-related findings reached policy before peer review.

## Impact Analysis

### On Scientific Progress

**Delayed Dissemination**

- Average 6-18 months from discovery to publication
- During COVID-19, traditional review timelines proved incompatible with public health urgency
- Preprints addressed speed but raised quality concerns
- Key findings in cancer, climate, and other urgent fields face similar delays

**Conservative Bias in Funding**

- NIH and NSF panels systematically favor incremental over transformative research
- Kaplan (2005) found proposals rated as "highly innovative" had *lower* funding success
- DARPA and HHMI models with program officer discretion produce higher-impact research per dollar
- Early-career scientists are most penalized by conservatism bias

### On Research Careers

**The Publish-or-Perish Treadmill**

- Tenure and promotion decisions heavily weight journal prestige
- Impact factor obsession drives researchers to target "glamour" journals (e.g., *Nature*, *Science*, *Cell*)
- Multiple rounds of rejection and resubmission consume years of career time
- Average first R01 grant at age 43 means productive early career years lost to grant writing

**Inequitable Burden**

- Women and minority scientists face higher rejection rates and longer review times
- Scientists at primarily undergraduate institutions have less time for resubmissions
- Non-native English speakers face linguistic bias in review
- Researchers without elite institutional affiliations face prestige bias

### On Public Trust

- High-profile retractions from peer-reviewed journals damage credibility
- COVID-era controversies (hydroxychloroquine, ivermectin, lab-leak) highlighted peer review failures
- Predatory journals exploiting the peer review brand undermine the concept
- Growing perception that peer review is a rubber stamp rather than genuine quality control

## Recent Developments

### eLife's Reviewed Preprint Model (2023-present)

In January 2023, *eLife* announced it would no longer make accept/reject decisions. Instead, it publishes reviewed preprints with public assessments and author responses. This experiment is being closely watched as a test of whether review can be decoupled from gatekeeping.

### NIH Study Section Reforms

- **FIRST (NIH)**: Programs to increase early-career investigator success rates
- **Reviewer training**: Expanded implicit bias training for study section members (implemented 2022)
- **Anonymous review pilots**: Testing removal of investigator biographical information from initial review
- **Payline adjustments**: Some institutes implementing special paylines for early-stage and new investigators

### OSTP Guidance on Public Access (2022-2025)

The August 2022 OSTP Nelson Memo requires that all federally funded research be freely available immediately upon publication by December 2025. This has implications for the peer review system:

- Journals can no longer justify subscription paywalls as the price of peer review
- Preprints with federal funding must be deposited in approved repositories
- Creates pressure to demonstrate the value-added of peer review

### AI-Assisted Review Tools

- Major publishers (Elsevier, Springer Nature) deploying AI tools for plagiarism detection, statistical checks, and reviewer matching
- ITHENTICATE and similar systems used in ~90% of major journals
- Emerging tools: StatReviewer for statistical methodology checks, SciScore for rigor criteria
- Concerns about AI-generated reviews submitted by overburdened reviewers

### Registered Reports Expansion

Over 300 journals now offer registered reports, where study design is peer-reviewed before data collection. This model:

- Eliminates publication bias (results cannot influence acceptance decision)
- Reduces questionable research practices
- Has shown no reduction in citation rates vs. traditional articles
- Remains a small fraction of total publications

## Current Reform Landscape

### Active Initiatives

| Initiative | Sponsor | Focus |
|------------|---------|-------|
| DORA (Declaration on Research Assessment) | Multi-stakeholder | End impact factor misuse in hiring/promotion |
| COPE (Committee on Publication Ethics) | Publisher consortium | Review ethics and standards |
| ASAPbio | Nonprofit | Preprint recognition and review |
| Peer Community In (PCI) | Academic-led | Free, transparent overlay review |
| ReviewerCredits | Nonprofit | Reviewer recognition platform |
| Publons (now Web of Science) | Clarivate | Reviewer recognition and tracking |

### Federal Agency Responses

| Agency | Reform | Status |
|--------|--------|--------|
| NIH | Implicit bias training for reviewers | Implemented 2022 |
| NIH | Anonymous review pilot | Testing phase |
| NSF | Broader impacts review improvement | Ongoing |
| DOE | Streamlined review for small grants | Implemented 2023 |
| DARPA | Program officer-driven model (no panels) | Longstanding alternative |

## Key Takeaways

1. **Volume outpaces capacity**: The number of manuscripts and grant proposals has grown far faster than the reviewer pool
2. **Bias is systemic**: Gender, institutional, geographic, and methodological biases are well-documented and persistent
3. **Delays are costly**: Months-to-years publication timelines cost lives in biomedical research and slow all fields
4. **Grant review is conservative**: Federal panels systematically favor safe, incremental research over innovation
5. **Reviewers are burned out**: Unpaid journal review labor is unsustainable; even compensated grant review is onerous
6. **Preprints are disrupting**: Preprint culture is growing but unevenly adopted across fields
7. **Impact factor distorts**: Journal prestige metrics warp career incentives and research priorities
8. **Reforms are fragmented**: Many experiments underway but no coordinated system-wide reform

## Sources

- Ioannidis, John P.A. "Why Most Published Research Findings Are False." *PLoS Medicine* 2, no. 8 (2005): e124. <https://doi.org/10.1371/journal.pmed.0020124>
- Ginther, Donna K., et al. "Race, Ethnicity, and NIH Research Awards." *Science* 333, no. 6045 (2011): 1015-1019. <https://doi.org/10.1126/science.1196783>
- Tomkins, Andrew, Min Zhang, and William D. Heavlin. "Reviewer Bias in Single- versus Double-Blind Peer Review." *PNAS* 114, no. 48 (2017): 12708-12713.
- Squazzoni, Flaminio, et al. "Peer Review and Gender Bias: A Study on 145 Scholarly Journals." *Science Advances* 7, no. 2 (2021): eabd0299.
- National Institutes of Health. "NIH Data Book: Success Rates." <https://report.nih.gov/nihdatabook/>
- National Science Foundation. "Merit Review Process Statistics." <https://www.nsf.gov/nsb/publications/pub_summ.jsp?ods_key=nsb2023>
- Publons. "Global State of Peer Review Report." <https://publons.com/community/gspr>
- Kovanis, Michail, et al. "The Global Burden of Journal Peer Review in the Biomedical Literature." *PLoS ONE* 11, no. 11 (2016): e0166387.

## Document Navigation

- Previous: [Overview](01-overview.md)
- Next: [History](03-history.md)
- Up: [Science](../01-overview.md)
