# Peer Review: Root Causes

## Overview

The failures of peer review are not random malfunctions but predictable consequences of structural incentives, institutional design, and human psychology. Understanding these root causes is essential for designing reforms that address problems at their source rather than treating symptoms.

## Structural Causes

### 1. Misaligned Incentives in Academic Publishing

The core structural problem is that the interests of publishers, authors, reviewers, and the scientific community are fundamentally misaligned.

| Stakeholder | Incentive | Effect on Review Quality |
|-------------|-----------|-------------------------|
| **Publishers** | Maximize journal prestige and revenue | Favor novel, sensational results; prioritize speed over rigor |
| **Authors** | Publish in high-impact journals for career advancement | Target prestige over appropriate venue; oversell findings |
| **Reviewers** | None (unpaid labor) | Minimal accountability; declining participation |
| **Institutions** | Rank faculty by publication metrics | Pressure to publish quantity over quality |
| **Funders** | Demonstrate return on investment | Favor "impact" stories; undervalue negative results |

**The fundamental misalignment**: Reviewers perform the core quality-control function but receive no compensation, no career credit, and no accountability for the quality of their reviews. Publishers capture the economic value of peer review ($10+ billion in annual academic publishing revenue) while externalizing the labor cost entirely.

### 2. The Publish-or-Perish System

The academic career structure creates relentless pressure to publish, which overwhelms the review system:

- **Tenure clock**: Junior faculty must produce a threshold of publications within 5-7 years
- **Quantity metrics**: Hiring committees count publications, often weighting journal prestige
- **Citation games**: Impact factor and h-index become career-defining numbers
- **Grant requirements**: Funded investigators must demonstrate "productivity" through publications
- **International competition**: Global talent pool means higher bars at every level

**Result**: The volume of submissions grows faster than the pool of qualified reviewers, degrading review quality system-wide.

### 3. The Journal Prestige Hierarchy

The hierarchy of journal prestige creates a cascading submission pattern that multiplies inefficiency:

```text
Submission: Nature/Science/Cell (reject 95%)
    |
    v
Resubmit: Nature sub-journals (reject 85%)
    |
    v
Resubmit: Top field journal (reject 70%)
    |
    v
Resubmit: Mid-tier field journal (reject 50%)
    |
    v
Resubmit: Lower-tier journal (accept ~60-70%)
```

- Each rejection-resubmission cycle requires new reviews
- The same work may be reviewed 6-12 times across journals
- Reviewers at lower-tier journals may review papers already rejected by experts at top journals
- Total review labor per paper is 3-5x what a single review would require

### 4. Oligopolistic Publishing Market

Five publishers (Elsevier, Springer Nature, Wiley, Taylor & Francis, SAGE) control approximately 50% of all published research. This concentration creates:

- **Barrier to reform**: Large publishers resist changes that threaten their business model
- **Lock-in effects**: Journals are tied to publisher brands, limiting editorial independence
- **Price inelasticity**: Academic libraries must subscribe regardless of cost
- **Reviewer exploitation**: Publishers profit from unpaid reviewer labor
- **Innovation resistance**: Established publishers have little incentive to adopt new review models

## Grant Review Causes

### 5. Conservative Bias in Study Sections

NIH study sections and NSF panels exhibit well-documented conservatism bias:

**Mechanisms**

| Mechanism | How It Works | Evidence |
|-----------|-------------|----------|
| **Risk aversion** | Reviewers penalize proposals without preliminary data | Nicholson & Ioannidis, *PLoS Biology* (2012) |
| **Methodological orthodoxy** | Novel methods rated as "risky" rather than "innovative" | Boudreau et al., *Management Science* (2016) |
| **Consensus pressure** | Discussion converges toward "safe" scores | Travis & Collins, *Nature* (1991) |
| **Expertise mismatch** | Truly novel proposals may not fit any existing study section | Kaplan, *Academic Medicine* (2005) |
| **Preliminary data requirement** | Catch-22: need funding to generate data, need data to get funding | NIH FIRST program acknowledgment |

**The Paradox of Innovation**: NIH and NSF mission statements emphasize supporting innovative research, but the review structures systematically select against it. The DARPA model (program officer discretion, tolerance for failure) consistently produces higher-impact research per dollar but is politically difficult to scale because it reduces "objectivity."

### 6. Demographic Homogeneity of Review Panels

Review panels do not reflect the diversity of the scientific workforce:

| Demographic | Share of Researchers | Share of NIH Reviewers | Gap |
|-------------|---------------------|----------------------|-----|
| Women | ~50% of PhDs | ~35% of study section chairs | Underrepresented in leadership |
| Black scientists | ~7% of PhDs | ~3% of study section members | Severely underrepresented |
| Hispanic scientists | ~9% of PhDs | ~5% of study section members | Underrepresented |
| Scientists at non-R1 institutions | ~60% of faculty | ~15% of reviewers | Dramatically underrepresented |
| International-origin scientists | ~40% of US workforce | Variable | Citizenship requirements limit participation |

**Consequence**: Homogeneous panels are more likely to fund research questions, methods, and investigators that resemble their own experience, perpetuating existing hierarchies.

### 7. Low Reliability of Grant Scoring

The NIH peer review system produces scores that are only modestly reliable:

- Pier et al. (2018) found that reassigning the same proposals to different study sections produced substantially different percentile scores
- The correlation between two independent reviews of the same proposal is approximately r = 0.2-0.3
- In practice, the difference between a funded and unfunded proposal often falls within the noise of reviewer disagreement
- NIH's own analysis suggests that proposals between the 10th and 30th percentile are essentially indistinguishable in quality

## Cognitive and Psychological Causes

### 8. Cognitive Biases in Evaluation

Peer review is vulnerable to the full range of cognitive biases affecting human judgment:

| Bias | Mechanism | Impact on Review |
|------|-----------|-----------------|
| **Confirmation bias** | Favoring evidence consistent with existing beliefs | Reviewers rate papers supporting their views more favorably |
| **Anchoring** | Over-reliance on first impression | Early reviewer scores disproportionately influence final decisions |
| **Halo effect** | Prestige of author/institution colors evaluation | Famous authors and elite institutions receive more favorable reviews |
| **Status quo bias** | Preference for established methods and theories | Novel approaches penalized relative to conventional ones |
| **Dunning-Kruger variants** | Overconfidence in areas outside expertise | Reviewers may critique methods they do not fully understand |
| **Availability bias** | Recent high-profile work influences standards | Trendy topics rated more favorably |

### 9. The Anonymity Paradox

Anonymity in review creates a fundamental tension:

**Benefits of Anonymity**

- Protects junior reviewers from retaliation by senior authors
- Allows honest negative assessments without social cost
- Reduces direct personal conflicts

**Costs of Anonymity**

- Enables lazy, unconstructive, or hostile reviews without accountability
- Allows bias to operate unchecked (reviewers face no consequences)
- Creates power asymmetry: reviewers know authors but not vice versa (single-blind)
- Permits conflicts of interest to go undetected

### 10. Reviewer Fatigue and Declining Quality

The unsustainable volume of review requests leads to predictable quality degradation:

- **Declining acceptance of review invitations**: From ~55% in 2010 to ~30-35% in 2025
- **Shorter, less thorough reviews**: Average review length and detail decreasing
- **Expertise mismatches**: When preferred reviewers decline, editors turn to less qualified alternatives
- **Speed-quality tradeoff**: Pressure to return reviews quickly conflicts with thorough evaluation
- **Reviewer recycling**: The same small pool of reviewers is repeatedly contacted, increasing burnout

## Institutional and Market Causes

### 11. The Impact Factor Distortion

The Journal Impact Factor (JIF) was designed as a library collection tool but has been repurposed as a measure of individual researcher quality:

**How Impact Factor Distorts**

- **Hiring decisions**: Committees count publications in "high-impact" journals
- **Funding decisions**: Grant reviewers assess applicant productivity by journal prestige
- **Editorial incentives**: Editors select for "citable" (sensational, novel) papers
- **Review incentives**: Reviewers impose higher standards at high-IF journals, creating self-fulfilling prestige
- **Author behavior**: Researchers target journal prestige rather than appropriate audience

**The Mathematical Problem**: JIF is a mean of a highly skewed distribution. A small number of heavily cited papers inflate the average while most articles in even the highest-impact journals are cited at or below the field average.

### 12. Lack of Accountability Infrastructure

No systematic mechanism exists to evaluate or improve reviewer quality:

- **No reviewer ratings**: Authors cannot rate the quality or helpfulness of reviews
- **No feedback loops**: Reviewers rarely learn the ultimate fate of papers they review
- **No standards**: What constitutes a "good" review is undefined and unmeasured
- **No training**: Most reviewers learn by doing, with no formal instruction
- **No consequences**: Poor reviews carry no professional penalty

### 13. Fragmented Reform Authority

No single entity has the authority to reform peer review system-wide:

| Potential Reformer | Authority | Limitation |
|--------------------|-----------|-----------|
| Individual journals | Own review process | Competitive pressure discourages unilateral reform |
| Publishers | Journal portfolios | Profit motive conflicts with reform |
| Funding agencies | Grant review only | No authority over journal review |
| Universities | Hiring/promotion criteria | No authority over journals or funders |
| Professional societies | Standards and norms | Advisory only; no enforcement |
| Congress | Federal funding conditions | Limited expertise; politicization risk |

**Result**: Everyone agrees the system needs reform, but no one has both the authority and the incentive to lead it.

## Systemic Interactions

### How Root Causes Reinforce Each Other

```text
Publish-or-perish --> More submissions --> Reviewer fatigue
       |                                       |
       v                                       v
Impact factor     --> Prestige bias  --> Conservative review
obsession                |                     |
       |                 v                     v
       +--------> Cascading rejections --> More total reviews
                         |
                         v
                  Delayed publication --> Career damage
                         |
                         v
                  More pressure to --> Publish-or-perish
                  publish quickly
```

### The Self-Reinforcing Cycle

1. Academic careers require publications in prestigious journals
2. Prestigious journals have high rejection rates, requiring multiple submission cycles
3. Multiple submission cycles multiply the demand for reviews
4. Increased review demand burns out reviewers and degrades review quality
5. Degraded review quality increases errors and bias
6. Errors and bias reduce public trust in peer-reviewed science
7. Reduced trust creates pressure for more stringent review
8. More stringent review further slows publication and increases reviewer burden
9. Return to step 1

## Summary of Root Causes

| Category | Root Cause | Severity | Tractability |
|----------|-----------|----------|-------------|
| Structural | Misaligned incentives (publisher profit vs. quality) | Critical | Medium |
| Structural | Publish-or-perish career system | Critical | Low (requires cultural shift) |
| Structural | Journal prestige hierarchy | High | Medium |
| Structural | Oligopolistic publishing market | High | Medium |
| Grant review | Conservative bias in study sections | High | Medium |
| Grant review | Demographic homogeneity of panels | High | Medium-High |
| Grant review | Low scoring reliability | High | Medium |
| Cognitive | Cognitive biases in evaluation | High | Low (inherent to human judgment) |
| Cognitive | Anonymity paradox | Medium | Medium |
| Cognitive | Reviewer fatigue | High | Medium-High |
| Institutional | Impact factor distortion | High | Medium |
| Institutional | Lack of accountability | High | Medium-High |
| Institutional | Fragmented reform authority | Critical | Low |

## Sources

- Pier, Elizabeth L., et al. "Low Agreement Among Reviewers Evaluating the Same NIH Grant Applications." *PNAS* 115, no. 12 (2018): 2952-2957.
- Boudreau, Kevin J., et al. "Looking Across and Looking Beyond the Knowledge Frontier: Intellectual Distance, Novelty, and Resource Allocation in Science." *Management Science* 62, no. 10 (2016): 2765-2783.
- Nicholson, Joshua M., and John P.A. Ioannidis. "Conform and Be Funded." *PLoS Biology* 10, no. 12 (2012): e1001456.
- Lariviere, Vincent, Stefanie Haustein, and Philippe Mongeon. "The Oligopoly of Academic Publishers in the Digital Era." *PLoS ONE* 10, no. 6 (2015): e0127502.
- Lee, Carole J., et al. "Bias in Peer Review." *Journal of the American Society for Information Science and Technology* 64, no. 1 (2013): 2-17.
- Kaplan, David. "How to Fix Peer Review." *The Scientist* 19, no. 1 (2005): 10.

## Document Navigation

- Previous: [History](03-history.md)
- Next: [Stakeholders](05-stakeholders.md)
- Up: [Science](../01-overview.md)
