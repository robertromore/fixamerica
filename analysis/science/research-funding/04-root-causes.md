# Research Funding: Root Causes

## Overview

The structural problems in federal research funding -- declining investment relative to GDP, abysmal grant success rates, geographic concentration, early-career researcher struggles, and chronic budget instability -- are not random failures or simple funding shortfalls. They emerge from deeply embedded features of the appropriations process, the university business model, the grant system's design, and the political economy of science funding. Understanding these root causes is essential for designing reforms that address systemic dysfunction rather than merely increasing dollar amounts.

## Root Cause 1: The Appropriations Process Is Structurally Hostile to Research

### Annual Budgeting for Multi-Year Work

Federal research funding is subject to the annual appropriations process, which creates a fundamental mismatch between the time horizons of science and politics:

- **Research projects** typically require 3-10 years to produce meaningful results
- **Appropriations** are made one year at a time, with no guarantee of continuation
- **Continuing resolutions** freeze funding at prior-year levels, blocking new grants and programs
- **Political cycles** shift priorities every 2-4 years, undermining long-term commitments

This mismatch means that researchers, institutions, and agencies cannot plan beyond the current fiscal year with confidence. The result is risk aversion, short-term thinking, and chronic underinvestment in the infrastructure and training that produce long-term returns.

### The Authorization-Appropriation Disconnect

The legislative process separates the decision to authorize programs (authorizing committees) from the decision to fund them (appropriations committees). This creates a structural incentive for over-promising and under-delivering:

- **Authorizing committees** gain credit for supporting research at no cost
- **Appropriations committees** face binding spending caps and must prioritize across all discretionary programs
- **Result**: Authorized levels consistently exceed appropriated amounts by 25-50%

The CHIPS and Science Act is the latest example: it authorized NSF at $15.6 billion for FY 2024, but appropriators provided $11.3 billion. This pattern has repeated with the America COMPETES Act (2007), its reauthorization (2010), and virtually every major research authorization since the 1980s.

### Discretionary Spending Squeeze

Federal research funding competes within the non-defense discretionary (NDD) budget, which has been shrinking as a share of total federal spending for decades:

| Category | 1970 Share | 2000 Share | 2024 Share |
|----------|-----------|-----------|-----------|
| Mandatory spending (entitlements) | 31% | 53% | ~63% |
| Defense discretionary | 40% | 16% | ~13% |
| Non-defense discretionary | 22% | 17% | ~14% |
| Net interest | 7% | 12% | ~10% |

Research funding is squeezed between growing mandatory programs (Social Security, Medicare, Medicaid), defense spending, and rising interest on the national debt. Without structural changes to how research is funded -- such as mandatory appropriations or trust funds -- the discretionary squeeze will continue to erode real research investment.

## Root Cause 2: The University Business Model Depends on Grant Revenue

### Research as Revenue Source

American research universities have evolved a business model that depends heavily on federal research grants -- not primarily for the research itself, but for the indirect cost recovery and prestige that grants generate:

- **Indirect costs** provide universities with unrestricted revenue averaging 53% of direct costs
- **Faculty positions** are often partially or wholly funded by grants (soft money)
- **University rankings** correlate with research expenditures, creating incentives to maximize grant volume
- **Graduate students** are funded through research assistantships on grants, making research training dependent on PI funding

This model creates several perverse incentives:

### Incentive to Expand Faculty Faster Than Funding

Universities have expanded the number of researchers competing for grants far faster than the funding pool has grown. Between 1998 and 2024, the number of NIH R01-equivalent applicants approximately doubled while the budget (in real terms) grew by roughly 40%. This expansion is rational for individual universities (each new faculty member brings indirect cost revenue) but collectively suicidal for success rates.

### Incentive to Maximize Indirect Cost Rates

Universities have a direct financial incentive to negotiate the highest possible F&A rate with their cognizant federal agency. Since F&A revenue is largely unrestricted, it cross-subsidizes university operations -- including activities with no connection to research. This creates a situation where a significant fraction of every federal research dollar never reaches the laboratory.

### Soft Money Dependency

At many institutions, researchers are expected to fund 50-100% of their own salaries from grants. This "soft money" model:

- Forces researchers to spend excessive time writing grants to maintain their positions
- Creates existential financial anxiety that discourages risk-taking
- Favors incremental, fundable research over transformative but risky ideas
- Disproportionately harms early-career researchers who have not yet built a funding portfolio

## Root Cause 3: The Grant System Discourages Risk-Taking

### Peer Review Conservatism

The peer review system -- science's gold standard for allocating funds -- systematically favors conservative, incremental research over high-risk, high-reward work:

- **Study sections** comprise established researchers who tend to value proven approaches
- **Preliminary data requirements** effectively require researchers to have already completed much of the proposed work before funding it
- **Scoring culture** penalizes uncertainty, novelty, and interdisciplinary approaches
- **Resubmission dynamics** reward applicants who give reviewers what they want rather than what might be transformative

A 2020 analysis published in *PNAS* found that NIH grants rated in the top 10% by innovation scores had lower funding rates than those in the top 10% by feasibility scores, demonstrating a systematic bias against novelty.

### Proposal Overload

With success rates at 20%, researchers must submit approximately five proposals to secure one grant. The cascading effects include:

- **Researcher time waste**: An estimated 40-50% of senior scientists' time is spent writing and reviewing grants
- **Reviewer fatigue**: Study section members face crushing volumes of proposals, reducing review quality
- **Marginal differentiation**: With many meritorious proposals competing, funding decisions increasingly depend on minor stylistic or presentational differences rather than scientific merit
- **System cost**: The total cost of preparing unfunded proposals has been estimated at $1.8 billion annually

### Replication of Safe Bets

The combination of low success rates and career consequences for unfunded researchers produces a "safe science" equilibrium:

- Researchers propose work they know they can accomplish rather than work that might fail instructively
- Grant applications promise specific deliverables rather than open-ended exploration
- Agencies fund expansions of existing knowledge rather than explorations of new territory
- The most creative and risky proposals are systematically filtered out

## Root Cause 4: Geographic Concentration Is Self-Reinforcing

### Cumulative Advantage

Research funding concentration follows a Matthew Effect ("to those who have, more shall be given"):

1. **Historical investment**: Early federal investments in the 1950s-1960s went to a small number of universities with existing capacity
2. **Infrastructure begets grants**: Universities with existing laboratories, equipment, and faculty attract more grants
3. **Talent migration**: Top researchers move to well-funded institutions, concentrating expertise
4. **Indirect cost revenue**: High-volume institutions generate more F&A revenue, enabling further infrastructure investment
5. **Network effects**: Peer reviewers tend to know and trust researchers at peer institutions
6. **Political reinforcement**: States with major research universities have stronger congressional advocates for research funding

### Institutional Stratification

The result is a three-tier system:

| Tier | Institutions | Share of Federal R&D | Characteristics |
|------|-------------|---------------------|-----------------|
| Top 20 | ~20 universities | ~30% | R1 doctoral, AAU members, >$1B research expenditures |
| Mid-tier | ~80 universities | ~50% | R1 doctoral, moderate research portfolios |
| Lower tier | ~900 institutions | ~20% | Smaller research programs, teaching focus |

### EPSCoR's Structural Limitations

The EPSCoR program, designed to address geographic disparities, is structurally incapable of doing so:

- Its budget (~$250M across all agencies) is less than 0.2% of total federal R&D
- It operates as a supplement to the competitive system rather than a redistribution mechanism
- Participating states must match federal funds, disadvantaging states with limited budgets
- Success metrics focus on increasing competitiveness within the existing system rather than changing the system itself

## Root Cause 5: Political Economy Favors Disease-Specific Over Structural Investment

### The Advocacy Imbalance

Specific diseases have well-organized advocacy groups (cancer patients, Alzheimer's caregivers, HIV/AIDS activists) that successfully lobby for increased funding for their disease. Basic research, research infrastructure, and systemic reforms have no comparable constituency:

- **Disease advocacy** produces earmarked funding increases for specific institutes (NCI, NIA)
- **Basic research** lacks a patient population to advocate for it
- **Indirect cost reform** is opposed by the powerful university lobby
- **Geographic redistribution** is opposed by states that currently benefit from concentration

### Congressional Dynamics

Congressional interest in research funding is driven by:

- **Parochial benefits**: Members support research institutions in their districts
- **Disease constituencies**: Members respond to patients and families affected by specific diseases
- **Bipartisan appeal**: Research funding is one of the few areas with genuine bipartisan support, but support is broad rather than deep
- **Appropriations hierarchy**: Research competes with housing, education, veterans, and other popular programs within the same spending category

### The Competitiveness Argument's Limitations

Advocates for increased research funding frequently invoke international competition (particularly with China) as a justification. While this argument resonates politically, it:

- Frames research as primarily a geopolitical tool rather than a public good
- Invites militarization of research priorities
- Creates security-driven restrictions on international collaboration
- Produces episodic surges (Sputnik, China Initiative) rather than sustained commitment

## Root Cause 6: Lack of Accountability for Research Outcomes

### Inputs Over Outputs

The federal research system measures itself primarily by inputs (dollars spent, grants awarded, proposals funded) rather than outputs (discoveries made, technologies developed, social benefits produced). This creates several problems:

- **No feedback loop**: There is no systematic mechanism to determine whether funded research produced its promised results
- **Volume incentive**: Agencies are rewarded for distributing more grants, not for funding better science
- **Publication as proxy**: The system uses publications and citations as proxies for research quality, despite widespread recognition that these metrics are deeply flawed
- **No failure analysis**: Unlike DARPA, which expects and learns from failure, the traditional grant system treats unfunded proposals as simply rejected rather than using them to improve the system

### Absence of Long-Term Evaluation

NIH and NSF conduct limited retrospective evaluation of funded research. Studies that have attempted to measure returns on investment -- such as analyses linking NIH-funded basic research to FDA-approved drugs -- are conducted by external researchers rather than built into the system. Without systematic evaluation, it is impossible to determine whether funding priorities, grant mechanisms, or review criteria are optimizing for scientific and social return.

## Root Cause Summary

| Root Cause | Primary Effect | Severity |
|-----------|---------------|----------|
| Annual appropriations for multi-year work | Budget instability, risk aversion | Critical |
| Authorization-appropriation disconnect | Chronic underfunding relative to commitments | Critical |
| Discretionary spending squeeze | Declining real investment | High |
| University business model dependency | Perverse incentives, indirect cost inflation | High |
| Grant system conservatism | Suppression of transformative research | High |
| Geographic concentration | Self-reinforcing inequality | Medium |
| Disease-specific advocacy dominance | Structural underinvestment in basic research | Medium |
| Lack of outcome accountability | No feedback loop for quality | Medium |

## Interaction Effects

These root causes are not independent. They interact in ways that amplify their individual effects:

- **Declining real funding** increases competition, which makes **peer review more conservative**, which produces **safer science**
- **University expansion** driven by **indirect cost incentives** increases the **applicant pool**, which reduces **success rates**, which increases **researcher time waste**
- **Geographic concentration** reinforces **political concentration** of research advocacy, which preserves the **status quo distribution**
- **Annual appropriations instability** makes **long-term planning impossible**, which makes **DARPA-style risk-taking** harder to implement across the system

Breaking out of this equilibrium requires addressing multiple root causes simultaneously rather than treating each in isolation.

## Document Navigation

- Previous: [History](03-history.md)
- Next: [Stakeholders](05-stakeholders.md)
- Up: [Science](../01-overview.md)
