# Reproducibility: History

## Overview

The concept of reproducibility has been central to the scientific method since its formalization, but recognition of a systemic "reproducibility crisis" is a relatively recent phenomenon. The history of reproducibility concerns can be traced through several phases: early philosophical foundations, the development of statistical methods, mounting evidence of problems, and the modern crisis and reform movement.

## Timeline

### Foundations of Scientific Replication (1600s-1900s)

- **1620**: Francis Bacon's *Novum Organum* establishes empirical observation and replication as foundations of the scientific method
- **1665**: *Philosophical Transactions of the Royal Society* becomes the first scientific journal, creating a record of methods and findings for others to evaluate and reproduce
- **1830s-1840s**: Charles Babbage identifies scientific misconduct categories in *Reflections on the Decline of Science in England* (1830), including "trimming" (smoothing data) and "cooking" (selecting favorable results)
- **1900-1930s**: R.A. Fisher develops the foundations of modern statistical testing, including the analysis of variance (ANOVA), randomized experiments, and the concept of statistical significance. Fisher initially proposed p < 0.05 as a convenient threshold for preliminary evidence, not a bright line for truth

### Formalization of Statistical Testing (1920s-1960s)

- **1928-1933**: Jerzy Neyman and Egon Pearson develop the framework of hypothesis testing with Type I and Type II errors, statistical power, and the rejection region. Their framework is merged with Fisher's approach in textbooks, creating the hybrid null hypothesis significance testing (NHST) procedure still dominant today
- **1935**: Fisher publishes *The Design of Experiments*, formalizing randomization and replication in experimental design
- **1959**: Sterling publishes one of the first studies documenting publication bias, finding that 97% of published psychology studies reported statistically significant results---a rate inconsistent with realistic effect sizes and statistical power
- **1962**: Jacob Cohen begins systematic work on statistical power in behavioral sciences, repeatedly demonstrating that most studies are dramatically underpowered

### Early Warnings (1960s-1990s)

- **1966**: Cohen's *Statistical Power Analysis for the Behavioral Sciences* demonstrates that the median power of studies in major psychology journals is below 50%, meaning they are more likely to miss true effects than detect them
- **1975**: The term "file drawer problem" is coined by Rosenthal (1979, published formally), describing how non-significant results are never published, creating systematic bias in the literature
- **1978**: Meehl publishes "Theoretical Risks and Tabular Asterisks," arguing that the ritual use of significance testing in psychology is scientifically unproductive
- **1985**: The International Committee of Medical Journal Editors (ICMJE) establishes the Uniform Requirements for Manuscripts, an early attempt at methods reporting standards
- **1994**: Dickersin and Min document publication bias in clinical trials, showing that studies with positive results are significantly more likely to be published
- **1996**: The CONSORT Statement establishes reporting standards for randomized controlled trials in medicine, the first major discipline-specific reporting guideline
- **1998**: Kerr formally describes HARKing (Hypothesizing After the Results are Known) and documents its prevalence in psychology

### The Crisis Emerges (2005-2012)

- **2005**: John Ioannidis publishes "Why Most Published Research Findings Are False" in *PLOS Medicine*. Using a Bayesian framework, Ioannidis demonstrates that when studies have low prior probability of a true effect, small sample sizes, analytical flexibility, and financial or career incentives, the majority of statistically significant findings will be false positives. The paper becomes one of the most cited in the history of medical research
- **2008**: The EQUATOR Network is established to promote the quality and transparency of health research reporting guidelines
- **2010**: Simmons, Nelson, and Simonsohn demonstrate "researcher degrees of freedom"---the many undisclosed choices researchers make in data collection and analysis---and show how easily false positive rates can be inflated to near 100% even without conscious fraud
- **2011**: Daryl Bem publishes "Feeling the Future" in the *Journal of Personality and Social Psychology*, a paper purporting to demonstrate precognition. The paper passes peer review at a top journal, sparking widespread debate about whether the field's methods are capable of distinguishing real effects from artifacts. Wagenmakers et al. publish a Bayesian reanalysis showing the data are better explained by bias than precognition
- **2011**: Simmons, Nelson, and Simonsohn publish "False-Positive Psychology: Undisclosed Flexibility in Data Collection and Analysis Allows Presenting Anything as Significant," formally demonstrating that common research practices can produce p < 0.05 results for any hypothesis
- **2011**: The Diederik Stapel fraud case is uncovered in the Netherlands. Stapel, a prominent social psychologist, is found to have fabricated data in dozens of published studies. While fraud rather than reproducibility per se, the case highlights the failure of the peer review and replication system to catch even outright fabrication for over a decade
- **2011**: Prinz, Schlange, and Asadullah (Bayer) report that only about 25% of published preclinical studies could be validated internally
- **2012**: Begley and Ellis (Amgen) report a replication rate of only 11% for landmark oncology studies

### The Reform Movement (2012-2018)

- **2012**: The Center for Open Science (COS) is incorporated by Brian Nosek and Jeffrey Spies at the University of Virginia, with initial funding from the Laura and John Arnold Foundation. COS becomes the institutional hub of the reproducibility reform movement
- **2013**: COS launches the Open Science Framework (OSF), a free platform for study pre-registration, data sharing, and research project management
- **2013**: The AllTrials campaign launches, demanding that all clinical trials be registered and their full results reported
- **2013**: The Registered Reports format is introduced at *Cortex*, where peer review occurs before data collection. Studies are accepted based on the importance of the question and the rigor of the methodology, regardless of whether results are statistically significant
- **2014**: NIH announces new requirements for rigor and reproducibility in grant applications, including requirements for authentication of key biological and chemical resources, consideration of sex as a biological variable, and enhanced methods reporting
- **2015**: The Open Science Collaboration publishes its landmark replication of 100 psychology studies in *Science*, finding only 36% replication success. The paper becomes a defining moment for the crisis, generating international media coverage and galvanizing reform
- **2015**: Freedman, Cockburn, and Simcoe estimate the annual cost of irreproducible preclinical research at $28 billion in the United States
- **2015**: The Transparency and Openness Promotion (TOP) Guidelines are published, providing a framework for journals to adopt reproducibility practices at varying levels of stringency
- **2016**: The NASEM report *Reproducibility and Replicability in Science* is commissioned (published 2019), providing a comprehensive framework and recommendations
- **2017**: Many Labs 2 begins data collection, eventually testing 28 effects across 125 samples in 36 countries
- **2018**: Benjamin et al. propose redefining statistical significance from p < 0.05 to p < 0.005 in *Nature Human Behaviour*, sparking intense debate
- **2018**: Camerer et al. replicate 21 social science studies published in *Nature* and *Science*, finding a 62% replication rate

### Institutional Response (2019-Present)

- **2019**: The National Academies of Sciences, Engineering, and Medicine (NASEM) publishes *Reproducibility and Replicability in Science*, providing authoritative definitions and a comprehensive framework for understanding and addressing the crisis
- **2019**: The American Statistical Association publishes a special issue of *The American Statistician* titled "Statistical Inference in the 21st Century: A World Beyond p < 0.05," with an editorial calling for the abandonment of the term "statistically significant"
- **2019**: NeurIPS introduces the ML Reproducibility Checklist for conference submissions
- **2020**: The COVID-19 pandemic accelerates both open science practices (rapid preprint sharing, data sharing) and reproducibility concerns (rushed peer review, retracted papers including high-profile retractions in *The Lancet* and *New England Journal of Medicine*)
- **2021**: The Reproducibility Project: Cancer Biology publishes its final results, finding 46% of tested effects replicated
- **2022**: The NIH Data Management and Sharing Policy is finalized (effective January 2023), requiring data sharing plans for all NIH-funded research
- **2022**: *eLife* adopts a publish-review-curate model, eliminating accept/reject decisions and emphasizing assessment of rigor
- **2022**: The Institute for Replication is founded to systematize replication in economics and social sciences
- **2023**: NIH Data Management and Sharing Policy takes effect
- **2024-2025**: Growing adoption of registered reports (350+ journals), pre-registration (130,000+ registrations on OSF), and open data practices, though adoption remains uneven across disciplines and institutions

## Key Turning Points

### Ioannidis (2005): The Mathematical Case

Ioannidis's paper provided the theoretical foundation for the reproducibility crisis by demonstrating mathematically that under realistic conditions, the majority of published positive findings are false. The paper's influence derived from its generality: it applied to any field where studies test many hypotheses, have limited sample sizes, allow analytical flexibility, or face publication bias. By framing the problem as a predictable consequence of incentives and methods rather than researcher malice, Ioannidis reframed the discussion from individual misconduct to systemic dysfunction.

### Open Science Collaboration (2015): The Empirical Evidence

While concerns about reproducibility had been accumulating for decades, the Open Science Collaboration's results provided the first large-scale empirical evidence of the scope of the problem. The study's publication in *Science* and its clear, quantifiable finding---only 36% replication---made the abstract concern concrete and undeniable. The project also demonstrated that large-scale collaborative replication was feasible and could be organized as a community effort.

### COVID-19 (2020): Crisis as Catalyst

The pandemic simultaneously demonstrated the value of open science (rapid sharing of sequences, preprints, and data enabled vaccine development at unprecedented speed) and its risks (premature claims, retracted papers, and public confusion). The high-profile retractions of studies in *The Lancet* and the *New England Journal of Medicine* based on data from Surgisphere---a company whose data could not be verified---illustrated the consequences of inadequate reproducibility practices when the stakes are highest.

## Historical Patterns

Several patterns emerge from this history:

- **Long lag between identification and action**: Concerns about statistical practices, publication bias, and underpowered studies were raised in the 1960s-1980s but did not produce systemic reform until the 2010s
- **Crisis-driven reform**: Major changes have been catalyzed by dramatic events (the Stapel fraud, the Bem precognition paper, the Open Science Collaboration results) rather than by the gradual accumulation of methodological critique
- **Discipline leaders and laggards**: Psychology and biomedicine have led the reform movement, while other fields have been slower to confront reproducibility challenges
- **Technology as enabler**: The internet, preprint servers, open-source statistical software, and platforms like OSF have made transparency and replication dramatically easier and cheaper than in the pre-digital era
- **Resistance to incentive change**: While awareness, tools, and norms have changed significantly, the fundamental career incentives of academic science---publications in high-impact journals as the currency of professional advancement---have proven remarkably resistant to reform

## References

- Ioannidis, John P. A. "Why Most Published Research Findings Are False." *PLOS Medicine* 2, no. 8 (2005): e124.
- Open Science Collaboration. "Estimating the Reproducibility of Psychological Science." *Science* 349, no. 6251 (2015): aac4716.
- Simmons, Joseph P., Leif D. Nelson, and Uri Simonsohn. "False-Positive Psychology: Undisclosed Flexibility in Data Collection and Analysis Allows Presenting Anything as Significant." *Psychological Science* 22, no. 11 (2011): 1359-1366.
- National Academies of Sciences, Engineering, and Medicine. *Reproducibility and Replicability in Science*. Washington, DC: National Academies Press, 2019.
- Cohen, Jacob. "The Statistical Power of Abnormal-Social Psychological Research: A Review." *Journal of Abnormal and Social Psychology* 65, no. 3 (1962): 145-153.
- Sterling, Theodore D. "Publication Decisions and their Possible Effects on Inferences Drawn from Tests of Significance." *Journal of the American Statistical Association* 54, no. 285 (1959): 30-34.
- Kerr, Norbert L. "HARKing: Hypothesizing After the Results are Known." *Personality and Social Psychology Review* 2, no. 3 (1998): 196-217.
- Begley, C. Glenn, and Lee M. Ellis. "Raise Standards for Preclinical Cancer Research." *Nature* 483, no. 7391 (2012): 531-533.

## Document Navigation

- Previous: [Current State](02-current-state.md)
- Up: [Science](../01-overview.md)
- Next: [Root Causes](04-root-causes.md)
