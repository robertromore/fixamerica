# Reproducibility: Resources

## Overview

This document provides a curated collection of key resources for understanding and addressing the reproducibility crisis, including seminal academic papers, institutional reports, organizations, tools, and educational materials.

## Seminal Papers

### Foundational Works

- Ioannidis, John P. A. "Why Most Published Research Findings Are False." *PLOS Medicine* 2, no. 8 (2005): e124. <https://doi.org/10.1371/journal.pmed.0020124>
    - The foundational paper demonstrating mathematically that under realistic conditions, the majority of published positive findings are false. The most-cited paper on the topic.

- Open Science Collaboration. "Estimating the Reproducibility of Psychological Science." *Science* 349, no. 6251 (2015): aac4716. <https://doi.org/10.1126/science.aac4716>
    - The landmark empirical study replicating 100 psychology experiments. Found only 36% replicated successfully.

- Simmons, Joseph P., Leif D. Nelson, and Uri Simonsohn. "False-Positive Psychology: Undisclosed Flexibility in Data Collection and Analysis Allows Presenting Anything as Significant." *Psychological Science* 22, no. 11 (2011): 1359-1366. <https://doi.org/10.1177/0956797611417632>
    - Demonstrates how researcher degrees of freedom inflate false positive rates, using a memorable example involving the Beatles song "When I'm Sixty-Four."

- Begley, C. Glenn, and Lee M. Ellis. "Raise Standards for Preclinical Cancer Research." *Nature* 483, no. 7391 (2012): 531-533. <https://doi.org/10.1038/483531a>
    - Reports Amgen's finding that only 11% of landmark cancer biology findings could be reproduced.

### Statistical Reform

- Benjamin, Daniel J., et al. "Redefine Statistical Significance." *Nature Human Behaviour* 2 (2018): 6-10. <https://doi.org/10.1038/s41562-017-0189-z>
    - Proposes lowering the significance threshold from p < 0.05 to p < 0.005.

- Lakens, Daniel, et al. "Justify Your Alpha." *Nature Human Behaviour* 2 (2018): 168-171. <https://doi.org/10.1038/s41562-018-0311-x>
    - Argues against a fixed threshold, proposing instead that researchers justify their chosen alpha level.

- Wasserstein, Ronald L., Allen L. Schirm, and Nicole A. Lazar. "Moving to a World Beyond 'p < 0.05.'" *The American Statistician* 73, sup1 (2019): 1-19. <https://doi.org/10.1080/00031305.2019.1583913>
    - The American Statistical Association's editorial calling for the abandonment of the term "statistically significant."

- Button, Katherine S., et al. "Power Failure: Why Small Sample Size Undermines the Reliability of Neuroscience." *Nature Reviews Neuroscience* 14 (2013): 365-376. <https://doi.org/10.1038/nrn3475>
    - Systematic analysis showing median statistical power in neuroscience is only 21%.

### Methodology and Reform

- Nosek, Brian A., Charles R. Ebersole, Alexander C. DeHaven, and David T. Mellor. "The Preregistration Revolution." *Proceedings of the National Academy of Sciences* 115, no. 11 (2018): 2600-2606. <https://doi.org/10.1073/pnas.1708274114>
    - Makes the case for pre-registration as a key tool for improving research credibility.

- Munaf√≤, Marcus R., et al. "A Manifesto for Reproducible Science." *Nature Human Behaviour* 1, no. 1 (2017): 0021. <https://doi.org/10.1038/s41562-016-0021>
    - Comprehensive overview of threats to reproducibility and measures to address them.

- Nosek, Brian A., et al. "Promoting an Open Research Culture." *Science* 348, no. 6242 (2015): 1422-1425. <https://doi.org/10.1126/science.aab2374>
    - Introduces the Transparency and Openness Promotion (TOP) Guidelines.

- Scheel, Anne M., et al. "An Excess of Positive Results: Comparing the Standard Psychology Literature with Registered Reports." *Advances in Methods and Practices in Psychological Science* 4, no. 2 (2021): 1-12. <https://doi.org/10.1177/25152459211007467>
    - Demonstrates that registered reports dramatically reduce the proportion of positive results, consistent with the elimination of publication bias.

### Economics of Irreproducibility

- Freedman, Leonard P., Iain M. Cockburn, and Timothy S. Simcoe. "The Economics of Reproducibility in Preclinical Research." *PLOS Biology* 13, no. 6 (2015): e1002165. <https://doi.org/10.1371/journal.pbio.1002165>
    - Estimates the annual cost of irreproducible preclinical research at $28 billion in the United States.

### Meta-Science and Incentives

- Smaldino, Paul E., and Richard McElreath. "The Natural Selection of Bad Science." *Royal Society Open Science* 3, no. 9 (2016): 160384. <https://doi.org/10.1098/rsos.160384>
    - Models how competitive academic environments select for researchers who produce impressive-looking but unreliable findings.

- Fanelli, Daniele. "'Positive' Results Increase Down the Hierarchy of the Sciences." *PLOS ONE* 5, no. 4 (2010): e10068. <https://doi.org/10.1371/journal.pone.0010068>
    - Documents the increasing rate of positive results over time and the relationship between field "softness" and positive result rates.

## Institutional Reports

- National Academies of Sciences, Engineering, and Medicine. *Reproducibility and Replicability in Science*. Washington, DC: National Academies Press, 2019. <https://doi.org/10.17226/25303>
    - The most authoritative U.S. report on the reproducibility crisis. Provides definitions, analysis, and recommendations.

- National Institutes of Health. "Rigor and Reproducibility." <https://www.nih.gov/research-training/rigor-reproducibility>
    - NIH's portal for reproducibility policies, training modules, and grant requirements.

- National Science Foundation. "Open Science." <https://www.nsf.gov/focus-areas/open-science>
    - NSF's open science policies and initiatives.

## Organizations

### Center for Open Science (COS)

- Website: <https://www.cos.io/>
- Mission: Increase openness, integrity, and reproducibility of research
- Key programs: Open Science Framework, TOP Guidelines, Registered Reports, SCORE project, badges
- Founded: 2013 by Brian Nosek and Jeffrey Spies

### EQUATOR Network

- Website: <https://www.equator-network.org/>
- Mission: Improve the quality and transparency of health research reporting
- Key resource: Database of 400+ reporting guidelines (CONSORT, STROBE, PRISMA, etc.)

### Berkeley Initiative for Transparency in the Social Sciences (BITSS)

- Website: <https://www.bitss.org/>
- Mission: Strengthen the quality of social science research through transparency and reproducibility
- Key programs: Workshops, curriculum, policy research

### Meta-Research Innovation Center at Stanford (METRICS)

- Website: <https://metrics.stanford.edu/>
- Mission: Study and improve the methods used in scientific research
- Founded by John Ioannidis

### Institute for Replication

- Website: <https://i4replication.org/>
- Mission: Systematize replication in economics and social sciences
- Key programs: Funded replications, replication databases

### Declaration on Research Assessment (DORA)

- Website: <https://sfdora.org/>
- Mission: Improve the ways research is evaluated, moving away from journal-based metrics
- Signatories: 3,000+ institutions and organizations

## Tools and Platforms

### Pre-Registration

| Tool | Description | URL |
|------|-------------|-----|
| Open Science Framework (OSF) | Free platform for pre-registration, data sharing, project management | <https://osf.io/> |
| AsPredicted | Streamlined pre-registration platform | <https://aspredicted.org/> |
| ClinicalTrials.gov | Required registration for clinical trials | <https://clinicaltrials.gov/> |
| PROSPERO | International prospective register of systematic reviews | <https://www.crd.york.ac.uk/prospero/> |

### Data Sharing

| Repository | Description | URL |
|------------|-------------|-----|
| Dryad | Curated data repository for research data | <https://datadryad.org/> |
| Figshare | Repository for figures, datasets, and other outputs | <https://figshare.com/> |
| Zenodo | CERN-hosted general-purpose open-access repository | <https://zenodo.org/> |
| Harvard Dataverse | Repository for research data across disciplines | <https://dataverse.harvard.edu/> |
| ICPSR | Social science data archive | <https://www.icpsr.umich.edu/> |

### Code Sharing and Computational Reproducibility

| Tool | Description | URL |
|------|-------------|-----|
| GitHub | Code hosting and version control | <https://github.com/> |
| Code Ocean | Computational reproducibility platform | <https://codeocean.com/> |
| Binder | Executable environments for shared code | <https://mybinder.org/> |
| ReproZip | Tool for packaging computational environments | <https://www.reprozip.org/> |

### Statistical Tools

| Tool | Description | URL |
|------|-------------|-----|
| statcheck | Automated statistical error detection | <https://statcheck.io/> |
| G*Power | Statistical power analysis software | <https://www.psychologie.hhu.de/arbeitsgruppen/allgemeine-psychologie-und-arbeitspsychologie/gpower> |
| JASP | Free Bayesian and frequentist statistics software | <https://jasp-stats.org/> |

## Educational Materials

### Courses and Curricula

- **FOSTER Open Science Training**: Free online training modules on open science practices. <https://www.fosteropenscience.eu/>
- **COS Open Science Training**: Workshops and materials from the Center for Open Science. <https://www.cos.io/communities/open-science-training>
- **NIH Rigor and Reproducibility Modules**: Training materials required for NIH-funded researchers. <https://www.nih.gov/research-training/rigor-reproducibility>
- **BITSS Curriculum**: Teaching materials on research transparency for university courses. <https://www.bitss.org/resource-library/>

### Books

- Ritchie, Stuart. *Science Fictions: How Fraud, Bias, Negligence, and Hype Undermine the Search for Truth*. New York: Metropolitan Books, 2020.
    - Accessible overview of the reproducibility crisis for general audiences.

- Harris, Richard. *Rigor Mortis: How Sloppy Science Creates Worthless Cures, Crushes Hope, and Wastes Billions*. New York: Basic Books, 2017.
    - Focuses on the biomedical reproducibility crisis and its consequences for patients.

- Chambers, Chris. *The Seven Deadly Sins of Psychology: A Manifesto for Reforming the Culture of Scientific Practice*. Princeton: Princeton University Press, 2017.
    - Analyzes the specific practices that undermine psychology and proposes reforms.

## Key Legislation and Policy

- **NIH Data Management and Sharing Policy** (effective January 2023): Requires data sharing plans for all NIH-funded research. <https://sharing.nih.gov/>
- **OSTP Nelson Memo** (August 2022): Directs federal agencies to ensure free, immediate public access to federally funded research results and data. <https://www.whitehouse.gov/ostp/news-updates/2022/08/25/ostp-issues-guidance-to-make-federally-funded-research-freely-available-without-delay/>
- **CONSORT Statement**: Reporting standards for randomized controlled trials. <https://www.consort-statement.org/>
- **TOP Guidelines**: Transparency and Openness Promotion guidelines for journals. <https://www.cos.io/initiatives/top-guidelines>

## Relevant Legal Citations

- 42 U.S.C. Chapter 6A, Subchapter III --- National Research Institutes (NIH authorizing statute)
- 42 U.S.C. Section 289a-1 --- Clinical research (requires clinical trial registration)
- 21 C.F.R. Part 11 --- Electronic records and signatures (FDA data integrity standards)
- OMB Circular A-110 --- Uniform administrative requirements for grants (data access provisions)

## Document Navigation

- Previous: [Roadmap](08-roadmap.md)
- Up: [Science](../01-overview.md)
- Next: [Actions](10-actions.md)
