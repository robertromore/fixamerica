# Reproducibility: Roadmap

## Overview

Addressing the reproducibility crisis requires a phased approach that builds momentum from low-hanging fruit toward systemic structural reform. This roadmap sequences interventions based on feasibility, impact, and the need to build coalitions and infrastructure before attempting more ambitious changes.

## Phase 1: Foundation Building (Years 1-2)

### Goals

- Expand adoption of existing reproducibility practices
- Strengthen infrastructure for transparency
- Build the evidence base for reform effectiveness
- Create political and institutional awareness

### Key Actions

#### 1.1 Expand Pre-Registration Infrastructure

- **Action**: Fund expansion of OSF, AsPredicted, and domain-specific registration platforms
- **Responsible parties**: NSF, NIH, private foundations (Arnold Ventures, Sloan Foundation)
- **Milestones**:
    - 200,000+ cumulative pre-registrations on OSF by end of Year 2
    - Pre-registration tools integrated into major institutional research management systems
    - Discipline-specific registration templates developed for fields currently underserved (engineering, chemistry, ecology)

#### 1.2 Journal Policy Adoption

- **Action**: Promote adoption of TOP Guidelines at Level 2 or higher across major journals
- **Responsible parties**: COS, professional societies, journal editors
- **Milestones**:
    - 50% of journals in psychology, neuroscience, and biomedical sciences adopt TOP Level 2+
    - Registered reports format available at 500+ journals
    - Major publishers (Elsevier, Springer Nature, Wiley) adopt standardized data availability enforcement

#### 1.3 Federal Funder Requirements

- **Action**: Strengthen enforcement of existing data sharing policies; expand pre-registration requirements
- **Responsible parties**: NIH, NSF, OSTP
- **Milestones**:
    - NIH Data Management and Sharing Policy compliance audited and enforced
    - NSF introduces pre-registration requirement for experimental research proposals
    - OSTP issues guidance on reproducibility practices across federal agencies

#### 1.4 Training Programs

- **Action**: Develop and disseminate reproducibility training for graduate students and postdoctoral fellows
- **Responsible parties**: Universities, professional societies, COS
- **Milestones**:
    - Open-access reproducibility curriculum available for adoption by graduate programs
    - 100+ universities incorporate reproducibility training into required coursework
    - Online training modules available through NIH OER and NSF-funded platforms

## Phase 2: Institutional Reform (Years 2-4)

### Goals

- Change hiring, tenure, and promotion criteria to reward rigor
- Create dedicated funding for replication
- Implement statistical reform at the journal level
- Build cross-disciplinary reproducibility infrastructure

### Key Actions

#### 2.1 Hiring and Tenure Reform

- **Action**: Promote DORA-aligned evaluation criteria that reward rigor, transparency, and open science practices over publication volume and journal prestige
- **Responsible parties**: Universities, DORA, professional societies, AAU
- **Milestones**:
    - 200+ departments adopt open science criteria in hiring and promotion guidelines
    - AAU issues model guidelines for research evaluation incorporating reproducibility
    - Federal funders include investigator open science track record in grant review criteria

#### 2.2 Dedicated Replication Funding

- **Action**: Establish dedicated funding programs for systematic replication of influential findings
- **Responsible parties**: NIH, NSF, private foundations
- **Milestones**:
    - NIH and NSF each launch pilot replication funding programs ($10-20 million annually)
    - Replication consortia established in at least 5 major fields
    - Registry of replication priorities maintained and updated annually

#### 2.3 Statistical Reporting Standards

- **Action**: Journals adopt enhanced statistical reporting requirements: mandatory effect sizes, confidence intervals, power analyses, and full reporting of all analyses conducted
- **Responsible parties**: Journal editors, professional societies (ASA, APA)
- **Milestones**:
    - Major psychology, neuroscience, and medical journals require effect sizes and confidence intervals
    - Pilot journals adopt Bayesian reporting requirements alongside NHST
    - ASA issues updated guidelines on statistical practice for specific disciplines

#### 2.4 Computational Reproducibility Infrastructure

- **Action**: Build infrastructure for automated verification of computational reproducibility
- **Responsible parties**: NSF, NIH, universities, COS
- **Milestones**:
    - Automated reproducibility checking available for major statistical software platforms (R, Python, Stata)
    - Major publishers pilot computational reproducibility verification for accepted papers
    - Standards for executable research artifacts established and adopted

## Phase 3: Systemic Transformation (Years 4-7)

### Goals

- Embed reproducibility practices as professional norms across all scientific disciplines
- Achieve comprehensive policy reform at the federal level
- Create sustainable infrastructure and funding for ongoing replication
- Transform the publication system to eliminate publication bias

### Key Actions

#### 3.1 Federal Legislation

- **Action**: Pass legislation mandating reproducibility practices for federally funded research
- **Responsible parties**: Congress, OSTP, federal agencies
- **Milestones**:
    - Research Reproducibility and Integrity Act introduced and passed
    - Federal agencies issue implementing regulations
    - Compliance monitoring established

#### 3.2 Publication System Reform

- **Action**: Transform the publication system to reward rigor over novelty and to eliminate publication bias
- **Responsible parties**: Journals, publishers, funders, professional societies
- **Milestones**:
    - Registered reports become a standard format at 1,000+ journals
    - Results-free peer review piloted at major journals across disciplines
    - Funder mandates require that results of all funded studies be made publicly available regardless of outcome

#### 3.3 International Coordination

- **Action**: Coordinate reproducibility standards and practices across national boundaries
- **Responsible parties**: OSTP, international science organizations, national funding agencies
- **Milestones**:
    - International accord on reproducibility standards for publicly funded research
    - Cross-national replication projects in key fields
    - Harmonized data sharing and pre-registration standards across major funding agencies

#### 3.4 Sustained Replication Infrastructure

- **Action**: Establish permanent institutions and funding for ongoing replication and verification
- **Responsible parties**: Federal agencies, universities, private foundations
- **Milestones**:
    - National Replication Initiative funded at $100+ million annually
    - Replication results systematically integrated into meta-analytic databases
    - Replication careers recognized and rewarded in the academic system

## Phase 4: Maturation and Evaluation (Years 7-10)

### Goals

- Assess the effectiveness of implemented reforms
- Refine practices based on evidence
- Ensure sustainability of reforms
- Extend practices to emerging fields

### Key Actions

#### 4.1 Comprehensive Assessment

- **Action**: Conduct large-scale assessments of whether implemented reforms have improved the reliability of published research
- **Responsible parties**: NASEM, COS, federal agencies
- **Milestones**:
    - New large-scale replication projects benchmark replication rates against pre-reform baselines
    - Analysis of whether pre-registered studies show improved replication rates compared to non-pre-registered studies
    - Assessment of whether statistical reform has reduced false positive rates in the literature

#### 4.2 Adaptive Refinement

- **Action**: Adjust policies and practices based on evidence of what works
- **Responsible parties**: Federal agencies, journals, institutions
- **Milestones**:
    - TOP Guidelines updated based on evidence of effectiveness at each level
    - Statistical reporting standards refined based on discipline-specific evidence
    - Training programs updated to reflect best practices

#### 4.3 Extension to Emerging Fields

- **Action**: Apply reproducibility practices to emerging research areas (AI/ML, synthetic biology, quantum computing) before crisis-level problems develop
- **Responsible parties**: Federal agencies, professional societies, journals
- **Milestones**:
    - Reproducibility standards established for AI/ML research
    - Pre-registration and data sharing norms adopted in emerging fields
    - Proactive rather than reactive approach to reproducibility in new disciplines

## Implementation Dependencies

```text
Phase 1: Foundation                Phase 2: Institution
(Infrastructure, adoption)        (Incentives, funding)
    |                                  |
    v                                  v
Pre-reg expansion ---------> Tenure/hiring reform
Journal policies  ---------> Replication funding
Funder requirements -------> Statistical standards
Training programs  ---------> Computation infra.
    |                                  |
    v                                  v
Phase 3: Systemic                 Phase 4: Maturation
(Legislation, transformation)     (Assessment, refinement)
    |                                  |
    v                                  v
Federal legislation -------> Comprehensive assessment
Publication reform ---------> Adaptive refinement
International coordination -> Extension to new fields
Sustained replication  -----> Long-term sustainability
```

## Key Risks and Mitigation

| Risk | Likelihood | Impact | Mitigation |
|------|-----------|--------|------------|
| Institutional inertia prevents incentive change | High | High | Focus on funder mandates that circumvent institutional resistance |
| Compliance without substance (performative adoption) | High | Medium | Develop enforcement mechanisms and audit tools |
| Political backlash against mandates | Medium | High | Frame reforms as efficiency and accountability measures |
| Researcher burnout from additional requirements | Medium | Medium | Invest in infrastructure that reduces compliance burden |
| Disciplinary resistance from non-experimental fields | Medium | Medium | Develop field-specific standards rather than one-size-fits-all |
| Insufficient funding for implementation | Medium | High | Build bipartisan support based on fiscal responsibility |
| Unintended consequences of statistical reform | Low | Medium | Pilot new standards before broad adoption |

## Success Metrics

| Metric | Baseline | Year 3 Target | Year 7 Target |
|--------|----------|---------------|---------------|
| Pre-registered studies (annual) | ~30,000 | 75,000 | 150,000 |
| Journals accepting registered reports | 350 | 750 | 1,500 |
| Replication rate in large-scale tests | ~40-60% | 55-65% | 65-75% |
| Departments with open science tenure criteria | ~50 | 200 | 500 |
| Federal replication funding (annual) | <$5 million | $30 million | $150 million |
| Studies with open data | ~20-30% | 50% | 80% |
| Researchers trained in reproducibility methods | ~10% | 30% | 60% |

## References

- Nosek, Brian A., et al. "Promoting an Open Research Culture." *Science* 348, no. 6242 (2015): 1422-1425.
- MunafÃ², Marcus R., et al. "A Manifesto for Reproducible Science." *Nature Human Behaviour* 1, no. 1 (2017): 0021.
- National Academies of Sciences, Engineering, and Medicine. *Reproducibility and Replicability in Science*. Washington, DC: National Academies Press, 2019.

## Document Navigation

- Previous: [Solutions](07-solutions.md)
- Up: [Science](../01-overview.md)
- Next: [Resources](09-resources.md)
