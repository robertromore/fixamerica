# Reproducibility: Actions

## Overview

Improving research reproducibility requires action from individuals at every level---from citizens and students to researchers, institutional leaders, and policymakers. This document outlines concrete steps each group can take to contribute to a more reliable scientific enterprise.

## For Citizens and Taxpayers

### Understand the Issue

- Learn about the reproducibility crisis and its implications for health care, public policy, and the responsible use of public funds
- Recognize that irreproducible research wastes an estimated $28 billion annually in the United States in preclinical research alone
- Understand that the problem is systemic, not primarily about individual fraud, and requires structural solutions

### Engage With Elected Officials

- Contact congressional representatives to support legislation requiring reproducibility practices for federally funded research
- Advocate for increased funding for replication studies and reproducibility infrastructure
- Support appropriations language directing NIH and NSF to strengthen reproducibility requirements
- Ask candidates about their positions on scientific integrity and accountability for research funding

### Evaluate Science Reporting Critically

- When encountering science news, look for indicators of reliability:
    - Was the study pre-registered?
    - Has it been replicated?
    - What is the sample size?
    - Is the data publicly available?
- Be cautious of single-study findings presented as definitive, especially from small samples or surprising claims
- Seek out journalists and outlets that cover the nuances of scientific evidence rather than hyping individual findings

### Support Reform Organizations

- Donate to or volunteer with organizations advancing reproducibility reform:
    - Center for Open Science (<https://www.cos.io/>)
    - Berkeley Initiative for Transparency in the Social Sciences (<https://www.bitss.org/>)
    - DORA (<https://sfdora.org/>)
- Share information about the reproducibility crisis with peers

## For Researchers (All Career Stages)

### Adopt Pre-Registration

- Pre-register confirmatory studies on OSF (<https://osf.io/>), AsPredicted (<https://aspredicted.org/>), or a domain-specific registry before collecting data
- Clearly distinguish between pre-registered (confirmatory) and exploratory analyses in all publications
- Use pre-registration templates specific to your research type and field
- Pre-register even when submitting to journals that do not require it; pre-registration benefits the researcher and the field regardless of journal policy

### Share Data and Code

- Make data underlying published findings publicly available, using repositories appropriate to your field (Dryad, Figshare, Zenodo, Harvard Dataverse, or domain-specific repositories)
- Share analysis code on GitHub or similar platforms, with sufficient documentation for others to understand and run it
- Use version control (Git) for all analysis scripts and document your computational environment
- Where data cannot be shared due to privacy constraints, share de-identified summaries, synthetic datasets, or documentation of how to access restricted data

### Improve Statistical Practices

- Conduct a priori power analyses for all studies and report them in publications
- Report effect sizes, confidence intervals, and exact p-values (not just "p < 0.05")
- Consider Bayesian alternatives or supplement frequentist analyses with Bayesian approaches
- Report all analyses conducted, not just those yielding significant results
- Distinguish between exploratory and confirmatory analyses in all publications
- Consult with a statistician for complex analyses

### Write Reproducible Methods

- Provide sufficient methodological detail for independent replication, including:
    - Exact protocols, reagent specifications, and equipment settings
    - Complete analysis pipelines, including preprocessing steps
    - Exclusion criteria determined before data collection
    - All dependent and independent variables measured
- Use reporting guidelines relevant to your research type (CONSORT, STROBE, PRISMA, etc.)
- Consider publishing detailed protocols on platforms like protocols.io

### Submit Registered Reports

- When conducting confirmatory research, submit as a registered report to a journal that offers the format
- Benefit from peer review of methods before data collection, when improvements can still be made
- Guarantee publication regardless of results, removing the incentive to produce significant findings

### Conduct and Publish Replications

- Dedicate a portion of your research effort to replication of important findings in your field
- Submit replication results for publication regardless of outcome
- Use platforms like ReplicationWiki (<https://replication.uni-goettingen.de/>) to document replication attempts
- Cite replication studies alongside original findings when discussing established effects

## For Graduate Students and Postdoctoral Fellows

### Build Skills Early

- Take courses or workshops on open science practices, reproducible research methods, and statistical best practices
- Learn version control (Git), literate programming (R Markdown, Jupyter notebooks), and data management
- Familiarize yourself with pre-registration, registered reports, and open data practices before your first major project
- Attend COS workshops, BITSS training, or university-offered open science courses

### Advocate Within Your Lab

- Discuss reproducibility practices with your advisor and lab group
- Propose adopting lab-level policies for pre-registration, data sharing, and code sharing
- Share resources and training materials with colleagues
- If your advisor is resistant, lead by example by adopting best practices in your own work

### Build Your Open Science Profile

- Pre-register your studies and share data and code from the start of your career
- Create an ORCID profile and link it to your pre-registrations, datasets, and publications
- Document your open science practices on your CV and in job applications
- Seek positions at institutions that value open science in hiring

## For Faculty and Lab Directors

### Set Lab Norms

- Establish lab-level requirements for pre-registration, data sharing, and code sharing
- Provide training and support for lab members to adopt these practices
- Budget time and resources for reproducibility practices in project planning
- Create lab protocols and checklists for rigorous research conduct

### Mentor for Rigor

- Train students and postdocs in statistical power analysis, pre-registration, and transparent reporting
- Model reproducibility practices in your own work
- Reward trainees who adopt rigorous practices, even when this slows publication output
- Discuss the reproducibility crisis openly and constructively in lab meetings

### Advocate for Institutional Change

- Serve on hiring and tenure committees and advocate for valuing rigor, transparency, and open science
- Propose changes to department-level evaluation criteria to include open science practices
- Support DORA adoption at your institution
- Advocate for university-level reproducibility infrastructure and training

## For Journal Editors and Reviewers

### Adopt and Enforce Transparency Policies

- Adopt TOP Guidelines at Level 2 or higher for your journal
- Require data availability statements and verify compliance
- Offer the registered reports format
- Implement open science badges for articles meeting transparency criteria
- Enforce reporting guidelines and checklists during peer review

### Review for Rigor

- Evaluate manuscripts based on the rigor of methods and the strength of evidence, not on the novelty or significance of results
- Check that statistical analyses are appropriate and fully reported
- Verify that sample sizes and power analyses are adequate
- Ask authors to share data and code as a condition of acceptance when possible
- Welcome replication studies and null results

### Reduce Publication Bias

- Accept articles based on the importance of the question and the rigor of the methods, regardless of whether results are significant
- Publish null results and replication studies
- Consider results-free peer review for initial submission decisions
- Make editorial decisions based on methodological quality rather than "impact"

## For University Administrators

### Reform Evaluation Criteria

- Adopt DORA and incorporate its principles into hiring, tenure, and promotion guidelines
- Include open science practices (pre-registration, data sharing, code sharing) as positive evaluation criteria
- Value replication studies and null results alongside novel findings
- Evaluate researchers on the rigor and transparency of their work, not just publication volume and journal prestige

### Build Infrastructure

- Fund and support institutional data repositories and research data management services
- Provide training in reproducibility practices for graduate students, postdocs, and faculty
- Establish open science centers or offices to support adoption of best practices
- Invest in computational infrastructure for reproducible research (container environments, shared computing resources)

### Create Incentives

- Offer awards or recognition for open science practices and reproducibility
- Include open science activities in workload models and annual reviews
- Provide course releases or supplements for faculty conducting replication studies
- Feature open science achievements in institutional communications

## For Policymakers and Congressional Staff

### Support Legislation

- Co-sponsor or support the Research Reproducibility and Integrity Act and the Scientific Transparency in Federally Funded Research Act
- Include reproducibility requirements in research authorization and appropriations bills
- Mandate pre-registration, data sharing, and replication provisions for federally funded research

### Increase Funding

- Appropriate dedicated funding for replication studies at NIH and NSF
- Fund reproducibility infrastructure (registries, platforms, training programs)
- Support the Center for Open Science and similar organizations through federal grants

### Require Accountability

- Direct GAO to audit federal agencies' implementation of reproducibility policies
- Require regular reporting on reproducibility metrics from NIH and NSF
- Hold hearings on the reproducibility crisis and the return on investment in publicly funded research

## Action Priority Matrix

| Action | Impact | Effort | Who |
|--------|--------|--------|-----|
| Pre-register studies | High | Low | All researchers |
| Share data and code | High | Medium | All researchers |
| Submit registered reports | Very high | Medium | Researchers (confirmatory) |
| Adopt TOP Guidelines | High | Medium | Journal editors |
| Reform tenure criteria | Very high | High | University administrators |
| Fund replication | High | High | Congress, funders |
| Learn open science skills | Medium | Low | Students, early-career |
| Contact elected officials | Medium | Low | Citizens |
| Enforce data availability | High | Medium | Journal editors |
| Pass reproducibility legislation | Very high | Very high | Congress |

## Document Navigation

- Previous: [Resources](09-resources.md)
- Up: [Science](../01-overview.md)
- Next: [Legislation](11-legislation.md)
