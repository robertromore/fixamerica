# Reproducibility: Opposition

## Overview

While the reproducibility crisis is widely acknowledged, many proposed reforms face significant opposition. This opposition ranges from principled disagreements about the nature of the problem and the effectiveness of proposed solutions to self-interested resistance from those who benefit from the status quo. Understanding the sources, arguments, and motivations of opposition is essential for designing reforms that can succeed.

## Sources of Opposition

### Researchers Whose Findings Are Challenged

**Who they are**: Established researchers whose published findings have failed to replicate, or who fear that their work may be targeted for replication.

**Arguments**:

- Replication failures may reflect poor execution by replicators, not problems with the original study
- Contextual differences between original and replication studies can explain discrepant results (the "hidden moderator" argument)
- Public replication failures cause reputational damage that is disproportionate to the epistemic value gained
- "Replication police" create a hostile environment that discourages creative, high-risk research

**Underlying motivations**: Professional reputation, career investment in challenged findings, fear of public embarrassment, legitimate concern about simplistic interpretations of replication failures.

**Assessment**: Some of these concerns have merit. Contextual sensitivity is real in some domains, and replication studies can be poorly executed. However, systematic replication projects have used careful protocols, pre-registration, and original-author consultation, and still found high failure rates. The "hidden moderator" argument, while sometimes valid, is often invoked post hoc without evidence and can become an unfalsifiable defense of any finding.

### Senior Faculty Resistant to Incentive Changes

**Who they are**: Tenured professors, department chairs, and senior administrators who built their careers under the current system and control hiring, tenure, and promotion decisions.

**Arguments**:

- Current metrics (publication count, h-index, journal impact factor) are imperfect but measurable; alternatives are vague
- Emphasizing replication and rigor over novelty could slow scientific progress
- The reproducibility crisis is overstated; science is self-correcting over time
- Mandating specific practices (pre-registration, data sharing) infringes on academic freedom

**Underlying motivations**: Institutional inertia, comfort with familiar evaluation criteria, concern that changing standards mid-career would devalue their own records, genuine uncertainty about how to evaluate researchers differently.

**Assessment**: The concern about measurability is legitimate---it is easier to count publications than to assess rigor. However, the argument that science is self-correcting over time is undermined by the evidence: self-correction has taken decades even for clearly false findings, and many fields show no systematic effort at replication at all.

### Journal Editors and Publishers

**Who they are**: Editors of high-impact journals and commercial publishers whose business models depend on publishing attention-grabbing, highly cited research.

**Arguments**:

- Registered reports and pre-registration reduce editorial flexibility to select the most impactful work
- Data sharing mandates create practical burdens for authors and publishers
- Journals already perform quality control through peer review
- Open-access requirements threaten the financial sustainability of publishing

**Underlying motivations**: Revenue models based on journal prestige and subscription access, editorial authority, legitimate concerns about the feasibility and cost of implementing new policies.

**Assessment**: Some journals have genuinely embraced reproducibility reforms (e.g., *eLife*, *Cortex*, PLOS journals). Others have adopted superficial compliance (requiring data availability statements without enforcing them). The fundamental tension is that the most prestigious journals became prestigious by publishing surprising results, and reproducibility reforms reduce the supply of such results.

### Researchers Concerned About Academic Freedom

**Who they are**: A diverse group of researchers who support reproducibility in principle but oppose mandated practices as overly prescriptive.

**Arguments**:

- Pre-registration is inappropriate for exploratory, hypothesis-generating research
- Requiring specific statistical methods constrains intellectual freedom
- One-size-fits-all policies do not account for disciplinary differences
- Government-mandated transparency requirements could enable adversaries to undermine legitimate research (e.g., in politically sensitive areas like climate science)

**Underlying motivations**: Genuine commitment to research autonomy, concern about bureaucratization of science, fear that inflexible rules will harm more than help.

**Assessment**: These concerns are often valid as applied to specific proposals and deserve careful consideration. Pre-registration is most appropriate for confirmatory research and should not be required for genuinely exploratory work. Statistical reform should expand the toolkit rather than mandate a single approach. However, the academic freedom argument is sometimes deployed strategically to resist any accountability for rigor.

### Researchers With Data Privacy Concerns

**Who they are**: Researchers working with human subjects data, proprietary datasets, or data covered by HIPAA, FERPA, or other privacy regulations.

**Arguments**:

- Open data mandates conflict with privacy protections for research participants
- Some data cannot be shared without re-identification risk
- Informed consent agreements may not have anticipated open data requirements
- Sharing raw data from vulnerable populations raises ethical concerns

**Underlying motivations**: Legitimate privacy and ethical concerns, protecting research participants, compliance with legal requirements.

**Assessment**: These concerns are genuine and important. However, they are often overstated. Privacy-preserving techniques (de-identification, synthetic data, secure data enclaves) can enable verification without full data release. The key is to distinguish between data that truly cannot be shared and data that researchers prefer not to share.

## Common Arguments Against Reform

### "The Crisis Is Overstated"

**Argument**: Replication rates vary widely across fields and methods. Some low replication rates reflect legitimate contextual differences rather than false findings. The alarm about reproducibility has been exaggerated and risks undermining public trust in science.

**Response**: While replication rates do vary, no field that has been systematically tested has shown rates consistent with the reliability of its published literature. Even in the best-performing fields (economics), roughly 30-40% of findings fail to replicate. The risk to public trust comes not from acknowledging the problem but from ignoring it.

### "Pre-Registration Stifles Creativity"

**Argument**: Requiring researchers to commit to hypotheses and analyses before collecting data prevents serendipitous discovery. Many important findings have emerged from unexpected patterns in data. Pre-registration imposes a rigid, confirmatory framework on inherently creative work.

**Response**: Pre-registration does not prohibit exploratory analysis. It requires that exploratory and confirmatory analyses be clearly distinguished. Researchers can pre-register confirmatory hypotheses and also report exploratory findings, labeled as such. The problem is not exploration but presenting exploration as confirmation.

### "Replication Is Not the Gold Standard"

**Argument**: In some fields (ecology, astronomy, anthropology), exact replication is impossible due to unique conditions, populations, or events. The emphasis on direct replication privileges experimental fields and marginalizes observational sciences.

**Response**: The NASEM report (2019) distinguished between "reproducibility" (computational: same data, same analysis, same result) and "replicability" (new data, same methods, consistent result). Both are important, and the relevant standard varies by discipline. Observational sciences can still improve through computational reproducibility, pre-registration of analysis plans, and triangulation across methods.

### "Open Data Creates Free Riders"

**Argument**: Researchers who invest years in building datasets should not be required to share them immediately, allowing others to publish analyses of their data without contributing to data collection.

**Response**: This concern can be addressed through embargo periods, citation requirements for data, and data use agreements. Many funders already allow a reasonable period of exclusive access before requiring data sharing.

### "Statistical Reform Is Too Disruptive"

**Argument**: Changing statistical thresholds or abandoning NHST would invalidate decades of literature, create confusion, and make it impossible to compare new findings with old ones.

**Response**: The existing literature is already of uncertain reliability. Raising statistical standards or adopting multiple inferential frameworks does not retroactively invalidate old research; it means that future research is held to a higher standard. The transition can be managed through dual-reporting (both old and new standards) during a transition period.

## Interest Group Opposition

| Group | Primary Objection | Strength of Opposition |
|-------|-------------------|----------------------|
| Researchers with challenged findings | Reputational harm | Strong but localized |
| Senior faculty (status quo) | Metric changes, incentive disruption | Moderate, widespread |
| Commercial publishers | Revenue and editorial control | Moderate |
| Academic freedom advocates | Mandated practices | Variable |
| Privacy-concerned researchers | Data sharing mandates | Moderate, legitimate |
| Researchers in non-experimental fields | Emphasis on exact replication | Moderate, legitimate |
| Researchers with proprietary data | Competitive advantage | Moderate |

## How Opposition Manifests

### Institutional Inertia

The most common form of opposition is not active resistance but passive inertia. Universities acknowledge reproducibility concerns but do not change hiring or tenure criteria. Journals add data availability statements but do not enforce them. Funders recommend best practices but do not mandate them. This pattern of symbolic adoption without substantive change allows institutions to appear responsive while preserving the status quo.

### Selective Compliance

Journals may adopt TOP Guidelines at the lowest level of stringency, claiming compliance while requiring minimal actual change. Researchers may pre-register studies with vague hypotheses that preserve analytical flexibility. Institutions may sign DORA but continue to evaluate faculty by impact factor. This selective compliance creates the appearance of reform without its substance.

### Rhetorical Resistance

Opposition is often framed in terms of widely shared values---academic freedom, scientific creativity, disciplinary autonomy, privacy protection---even when the underlying motivation is resistance to accountability. This framing makes it difficult to distinguish legitimate concerns from self-interested obstruction.

## Implications for Reform Strategy

Effective reforms must:

- **Address legitimate concerns**: Privacy protections, disciplinary differences, and the value of exploratory research deserve accommodation
- **Create positive incentives**: Reforms that reward good practices (registered reports guaranteeing publication, badges for open data) are more effective than punitive mandates
- **Target gatekeepers**: Funders and journals have the most leverage to change researcher behavior, because they control the resources researchers need
- **Build coalitions**: Early-career researchers, reform organizations, and funders share interests in reform and can collectively overcome institutional inertia
- **Provide transition support**: Training, infrastructure, and gradual implementation reduce the costs of adoption and address concerns about disruption

## References

- Gilbert, Daniel T., et al. "Comment on 'Estimating the Reproducibility of Psychological Science.'" *Science* 351, no. 6277 (2016): 1037.
- Anderson, Christopher J., et al. "Response to Comment on 'Estimating the Reproducibility of Psychological Science.'" *Science* 351, no. 6277 (2016): 1037.
- Nosek, Brian A., Charles R. Ebersole, Alexander C. DeHaven, and David T. Mellor. "The Preregistration Revolution." *Proceedings of the National Academy of Sciences* 115, no. 11 (2018): 2600-2606.
- National Academies of Sciences, Engineering, and Medicine. *Reproducibility and Replicability in Science*. Washington, DC: National Academies Press, 2019.

## Document Navigation

- Previous: [Stakeholders](05-stakeholders.md)
- Up: [Science](../01-overview.md)
- Next: [Solutions](07-solutions.md)
