# Social Media Platform Accountability Act

This proposal establishes a federal framework for social media platform accountability regarding demonstrably false information, particularly concerning elections and public health, while safeguarding free speech.

---

## Purpose

Establish a Social Media Platform Accountability Framework that:

- Defines clear legal responsibilities for social media platforms regarding the amplification of demonstrably false information.
- Protects the integrity of democratic processes and public health by mitigating the spread of harmful disinformation.
- Ensures transparency in content moderation and algorithmic amplification.
- Safeguards free speech by focusing on demonstrably false information and providing robust appeal mechanisms.

---

## Part I: Definitions and Scope

### Section 1. Definitions

(a) **Social Media Platform.** An online service that primarily enables users to communicate and interact with each other, and to create, share, and discover user-generated content, and that has more than 50 million monthly active users in the United States.

(b) **Demonstrably False Information.** Information that has been factually disproven by at least two independent, non-partisan fact-checking organizations certified by the Independent Election Commission (IEC), or by a court of law, or by a federal agency with relevant scientific expertise (e.g., CDC for public health, NIST for election technology), and where the falsity is material to the integrity of elections or public health.

(c) **Algorithmic Amplification.** The process by which a platform's algorithms or automated systems increase the visibility, reach, or engagement of content beyond its organic distribution.

### Section 2. Scope

(a) This Act applies to demonstrably false information directly related to:
    - (1) Federal elections, including voter registration, voting procedures, candidate eligibility, and election results.
    - (2) Public health emergencies, including the efficacy and safety of vaccines, treatments, and public health interventions as certified by the CDC or Surgeon General.

(b) This Act does not apply to:
    - (1) Opinion, satire, parody, or artistic expression.
    - (2) Information that is merely misleading or incomplete, but not demonstrably false.
    - (3) Information related to political campaigns that does not involve demonstrably false claims about election integrity or public health.

---

## Part II: Platform Responsibilities

### Section 11. Transparency in Amplification

(a) Social Media Platforms shall publicly disclose:
    - (1) The general principles and parameters of their algorithmic amplification systems.
    - (2) Data on the reach and engagement of demonstrably false information identified under this Act, including how it was amplified.
    - (3) Metrics on content moderation actions taken under this Act, including removal rates and appeals.

### Section 12. Expedited Takedown and Labeling

(a) Upon notification by the IEC that specific content constitutes demonstrably false information under this Act, platforms shall:
    - (1) Within 24 hours, cease algorithmic amplification of such content.
    - (2) Within 48 hours, apply a clear and prominent label to the content indicating its demonstrably false nature, with a link to verified factual information.
    - (3) Within 72 hours, remove the content if it continues to be algorithmically amplified despite labeling, or if it poses an imminent and severe threat to public safety (e.g., inciting violence related to election fraud, promoting dangerous unproven health remedies).

(b) Platforms shall prioritize actions that reduce the spread of demonstrably false information over outright removal, consistent with free speech principles.

### Section 13. Algorithmic Auditing

(a) Platforms shall conduct regular, independent audits of their algorithmic amplification systems to assess their impact on the spread of demonstrably false information.

(b) Audit reports shall be submitted to the IEC and a redacted version made public annually.

---

## Part III: Oversight and Enforcement

### Section 21. Independent Election Commission (IEC) Role

(a) The IEC shall be responsible for:
    - (1) Certifying independent, non-partisan fact-checking organizations, per the `Independent Fact-Checking Integration Act`.
    - (2) Receiving and reviewing reports of demonstrably false information.
    - (3) Notifying platforms of content identified as demonstrably false under this Act.
    - (4) Overseeing platform compliance with transparency and moderation requirements.

### Section 22. Appeals Process

(a) Platforms shall establish a clear and transparent appeals process for users whose content is labeled or removed under this Act.

(b) Users shall have the right to appeal to an independent oversight board (e.g., a platform's existing oversight board, or a new independent body established for this purpose) whose decisions are binding on the platform.

### Section 23. Penalties

(a) Platforms that fail to comply with the requirements of this Act shall be subject to civil penalties of up to $1,000,000 per violation, or 0.1% of global annual revenue, whichever is greater.

(b) Penalties shall be assessed by the IEC and may be appealed to federal court.

---

## Hostile Reinterpretation Stress Test

### Attack Vector 1: Censorship Claims and Free Speech Infringement

**Attack:** Platforms or political actors claim the Act leads to censorship and infringes on free speech, using it to suppress legitimate political discourse.

**Countermeasure:**

- **Narrow Definition of "Demonstrably False Information":** Section 1(b) requires factual disproof by multiple independent sources or federal agencies, and materiality to elections/public health, explicitly excluding opinion, satire, or merely misleading content.
- **Prioritization of Labeling over Removal:** Section 12(b) prioritizes labeling to preserve content, with removal reserved for severe threats or persistent amplification.
- **Robust Appeals Process:** Section 22 provides for independent oversight and binding decisions, protecting users' rights.
- **Judicial Review:** Penalties and IEC decisions are subject to federal judicial review.

**Outcome:** The Act is narrowly tailored to address demonstrably false information with high public impact, while providing multiple safeguards for free speech and due process, making broad censorship claims difficult to sustain.

---

### Attack Vector 2: Platform Non-Compliance or Evasion

**Attack:** Platforms fail to comply with transparency, labeling, or removal requirements, or find loopholes to continue amplifying disinformation.

**Countermeasure:**

- **Significant Civil Penalties:** Section 23(a) imposes substantial financial penalties, creating a strong deterrent.
- **Algorithmic Auditing:** Section 13 mandates independent audits to detect evasion in amplification systems.
- **Transparency Requirements:** Section 11 requires public disclosure of amplification and moderation data, allowing public and expert scrutiny.
-   **IEC Oversight:** The IEC has clear authority to monitor and enforce compliance.

**Outcome:** The combination of financial penalties, transparency, and independent auditing makes systematic non-compliance or evasion difficult and costly for platforms.

---

### Attack Vector 3: Partisan Capture of Fact-Checking or IEC

**Attack:** Political actors attempt to capture the IEC or certified fact-checking organizations to label legitimate content as "demonstrably false" for partisan advantage.

**Countermeasure:**

-   **IEC Independence:** The IEC's independent composition (established by the Independent Election Commission Act) protects against partisan capture.
- **Multiple Fact-Checkers:** Section 1(b) requires disproof by *at least two independent, non-partisan* fact-checking organizations, reducing reliance on a single entity.
-   **Clear Certification Criteria:** The IEC must establish objective criteria for certifying fact-checking organizations, ensuring non-partisanship.
-   **Judicial Review:** IEC decisions on content and penalties are subject to federal judicial review.

**Outcome:** The multi-layered requirement for independent, non-partisan verification and robust judicial review makes partisan capture of the labeling process difficult and challengeable.

---

## Residual Vulnerabilities

| Vulnerability | Severity | Mitigation |
|:---|:---|:---|
| **Defining "materiality" in practice** | Medium | IEC rulemaking with public comment; judicial precedent will refine over time. |
| **Resource intensity for IEC oversight** | Medium | Adequate funding for IEC; collaboration with academic researchers for auditing. |
| **Global nature of platforms vs. national law** | High | Encourage international cooperation on platform regulation; focus on domestic impact. |

---

## Related Documents

- [Independent Election Commission Act](independent-election-commission-act.md) - Establishes the IEC, which oversees this Act.
- [Federal Digital Literacy and Media Education Program Act](federal-digital-literacy-act.md) - Complementary program to address disinformation.

---

## Document Navigation

- Parent: [Regional Federalism Proposals](../../README.md)
